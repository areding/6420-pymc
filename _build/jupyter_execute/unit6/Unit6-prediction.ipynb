{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5554edbc",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253075e",
   "metadata": {},
   "source": [
    "# Prediction*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246df40f",
   "metadata": {},
   "source": [
    "We previously covered Bayesian prediction in Unit 4, Lesson 13.  Recall that the core of Bayesian prediction is the *posterior predictive distribution*\n",
    "\n",
    "$$\n",
    "f(x_{n+1}\\vert x_{1} , \\cdots , x_{n}) = \\int_{\\Theta}f(x_{n+1} \\vert \\mathbf{\\theta})\\pi(\\theta \\vert x_{1} , \\cdots , x_{n})d\\theta\n",
    "$$\n",
    "\n",
    "Think of this as a \"weighted average\" of the likelihood for new data, with \"weights\" prescribed by for $\\pi(\\theta \\vert x_{1} , \\cdots , x_{n})$.  This is a departure from the \"plug-in predictions\" you may have encountered in a frequentist course.  The result is a distribution of predictions.  It should be clear by now that analytical methods are often intractible, and that is no different for the posterior predictive distribution.  Fortunately, `pymc` has some tools that make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30135d",
   "metadata": {},
   "source": [
    "## Taste of Cheese*\n",
    "\n",
    "Adapted from [Unit 6: cheese.odc](https://raw.githubusercontent.com/areding/6420-pymc/main/original_examples/Codes4Unit6/cheese.odc).\n",
    "\n",
    "The link in the original .odc file is dead. I downloaded the data from [here](https://www3.nd.edu/~busiforc/handouts/Data%20and%20Stories/multicollinearity/Cheese%20Taste/Cheddar%20Cheese%20Data.html) and have a copy [here](https://raw.githubusercontent.com/areding/6420-pymc/main/data/cheese.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984bec3",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As cheddar cheese matures, a variety of chemical processes take place. The taste of matured cheese is related to the concentration of several chemicals in the final product. In a study of cheddar cheese from the Latrobe Valley of Victoria, Australia, samples of cheese were analyzed for their chemical composition and were subjected to taste tests. Overall taste scores were obtained by combining the scores from several tasters.\n",
    "\n",
    "Can the score be predicted well by the predictors: Acetic, H2S, and Lactic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd94c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/cheese.csv\", index_col=0)\n",
    "X = data[[\"Acetic\", \"H2S\", \"Lactic\"]].to_numpy()\n",
    "# add intercept column to X\n",
    "X_aug = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "y = data[\"taste\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72873fa-fea5-4bbe-b488-4f0c1ffd77be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taste</th>\n",
       "      <th>Acetic</th>\n",
       "      <th>H2S</th>\n",
       "      <th>Lactic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.3</td>\n",
       "      <td>4.543</td>\n",
       "      <td>3.135</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.159</td>\n",
       "      <td>5.043</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.0</td>\n",
       "      <td>5.366</td>\n",
       "      <td>5.438</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.9</td>\n",
       "      <td>5.759</td>\n",
       "      <td>7.496</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.6</td>\n",
       "      <td>4.663</td>\n",
       "      <td>3.807</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   taste  Acetic    H2S  Lactic\n",
       "1   12.3   4.543  3.135    0.86\n",
       "2   20.9   5.159  5.043    1.53\n",
       "3   39.0   5.366  5.438    1.57\n",
       "4   47.9   5.759  7.496    1.81\n",
       "5    5.6   4.663  3.807    0.99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3467bf7",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta, tau]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd3991c049849b18f3aff0570289048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 11 seconds.\n",
      "Sampling: [taste_score]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e762bc70d3224b2b9d3ca4a0cfbd28a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pm.Model() as m:\n",
    "    # associate data with model (this makes prediction easier)\n",
    "    X_data = pm.Data(\"X\", X_aug)\n",
    "    y_data = pm.Data(\"y\", y)\n",
    "\n",
    "    # priors\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1000, shape=X.shape[1] + 1)\n",
    "    tau = pm.Gamma(\"tau\", alpha=0.001, beta=0.001)\n",
    "    sigma = pm.Deterministic(\"sigma\", 1 / pm.math.sqrt(tau))\n",
    "\n",
    "    mu = pm.math.dot(X_data, beta)\n",
    "\n",
    "    # likelihood\n",
    "    pm.Normal(\"taste_score\", mu=mu, sigma=sigma, observed=y_data, shape=X_data.shape[0])\n",
    "\n",
    "    # start sampling\n",
    "    trace = pm.sample(5000, target_accept=0.95)\n",
    "    pm.sample_posterior_predictive(trace, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f1b9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>-28.594</td>\n",
       "      <td>20.567</td>\n",
       "      <td>-68.302</td>\n",
       "      <td>12.992</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.176</td>\n",
       "      <td>7366.0</td>\n",
       "      <td>9567.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1]</th>\n",
       "      <td>0.219</td>\n",
       "      <td>4.635</td>\n",
       "      <td>-8.825</td>\n",
       "      <td>9.345</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>6929.0</td>\n",
       "      <td>8886.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2]</th>\n",
       "      <td>3.909</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.322</td>\n",
       "      <td>6.350</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>8958.0</td>\n",
       "      <td>10953.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3]</th>\n",
       "      <td>19.914</td>\n",
       "      <td>8.862</td>\n",
       "      <td>2.569</td>\n",
       "      <td>37.440</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.069</td>\n",
       "      <td>9970.0</td>\n",
       "      <td>11208.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10522.0</td>\n",
       "      <td>10951.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>10.446</td>\n",
       "      <td>1.529</td>\n",
       "      <td>7.770</td>\n",
       "      <td>13.536</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>10522.0</td>\n",
       "      <td>10951.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean      sd  hdi_2.5%  hdi_97.5%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "beta[0] -28.594  20.567   -68.302     12.992      0.240    0.176    7366.0   \n",
       "beta[1]   0.219   4.635    -8.825      9.345      0.056    0.041    6929.0   \n",
       "beta[2]   3.909   1.283     1.322      6.350      0.014    0.010    8958.0   \n",
       "beta[3]  19.914   8.862     2.569     37.440      0.089    0.069    9970.0   \n",
       "tau       0.010   0.003     0.005      0.015      0.000    0.000   10522.0   \n",
       "sigma    10.446   1.529     7.770     13.536      0.015    0.014   10522.0   \n",
       "\n",
       "         ess_tail  r_hat  \n",
       "beta[0]    9567.0    1.0  \n",
       "beta[1]    8886.0    1.0  \n",
       "beta[2]   10953.0    1.0  \n",
       "beta[3]   11208.0    1.0  \n",
       "tau       10951.0    1.0  \n",
       "sigma     10951.0    1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e84c05-bb37-429f-8be5-90b811205304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.577111\n",
       "r2_std    0.075772\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = trace.posterior_predictive.stack(sample=(\"chain\", \"draw\"))[\n",
    "    \"taste_score\"\n",
    "].values.T\n",
    "az.r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffb987",
   "metadata": {},
   "source": [
    "PyMC may give warnings about the model unless we increase the ```target_accept``` parameter of ```pm.sample```. This is because PyMC uses more diagnostics to check if there are any problems with its exploration of the parameter space. Divergences indicate bias in the results.\n",
    "\n",
    "For further reading, check out [Diagnosing Biased Inference with Divergences](https://docs.pymc.io/en/v3/pymc-examples/examples/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0941c",
   "metadata": {},
   "source": [
    "It looks like there are multiple ways to get predictions on out-of-sample data in PyMC. The easiest way is to set up a shared variable using pm.Data in the original model, then using pm.set_data to change to the new observations before calling pm.sample_posterior_predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e01d45",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [taste_score]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2876c4206e7d4ccfa5f182697aa88227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# single prediction on out-of-sample data\n",
    "new_obs = np.array([[1.0, 5.0, 7.1, 1.5]])\n",
    "pm.set_data({\"X\": new_obs}, model=m)\n",
    "ppc = pm.sample_posterior_predictive(trace, model=m, predictions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f88ce-fdad-4e97-9aa3-4387b4e3b389",
   "metadata": {},
   "source": [
    "The default behavior now is to give one prediction per $y$-value. The professor often asks for a single prediction based on the new data; the equivalent here would be to take the mean of the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf0b18eb-c91f-492b-b726-b293834fc9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taste_score[0]</th>\n",
       "      <td>30.055</td>\n",
       "      <td>11.197</td>\n",
       "      <td>8.671</td>\n",
       "      <td>52.698</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.063</td>\n",
       "      <td>17533.0</td>\n",
       "      <td>17753.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean      sd  hdi_2.5%  hdi_97.5%  mcse_mean  mcse_sd  \\\n",
       "taste_score[0]  30.055  11.197     8.671     52.698      0.085    0.063   \n",
       "\n",
       "                ess_bulk  ess_tail  r_hat  \n",
       "taste_score[0]   17533.0   17753.0    1.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(ppc.predictions, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef2e78",
   "metadata": {},
   "source": [
    "## Intuition: From posterior trace to posterior predictive summary:\n",
    "\n",
    "Let's \"manually\" simulate the posterior predictive distribution from the posterior samples in `trace`: without the help of `pm.sample_posterior_predictive`.\n",
    "\n",
    "### Understanding the PyMC Posterior Trace Structure:\n",
    "When we fit a model in PyMC, the results are stored in an arviz.InferenceData object: what we saved as `trace` above. For each parameter, PyMC stores the samples as an xarray DataArray with two main sampling dimensions:\n",
    "\n",
    " - `chain`: The MCMC chain index (typically 4 chains by default)\n",
    "\n",
    " - `draw`: The sequential draw within each chain (e.g., 5000 in this case: as determined by our use of `draws=5000`)\n",
    "\n",
    "Specifically:\n",
    "\n",
    " - Since `beta` is vector-valued in our implementation above: the shape of beta within `trace` is `(num_chains,num_draws,num_coefficients)`, IE : `(4,5000,4)`\n",
    "\n",
    " - Since `sigma` is scalar valued its shape is `(4,5000,1)`\n",
    "\n",
    "To work with these samples in a vectorized manner, we flatten the chain and draw dimensions into a single “samples” dimension to make them easier to work with.  The resulting shapes are:\n",
    "\n",
    " - `posterior_betas` has shape `(4,20000)`\n",
    " \n",
    " - `posterior_sigmas` has shape `(1,20000)`\n",
    "\n",
    "This is done using `xarray.DataArray.stack`.  You can check out the documentation [here](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.stack.html) if you need more guidance.  adding on `.values` converts the resulting xarray to a NumPy array.\n",
    "\n",
    "### Pre-Selecting Random Joint Posterior Indices:\n",
    "\n",
    "To draw from the joint posterior, we first predefine a list of random indices—each corresponding to a complete joint sample of all model parameters.  Each entry in `indices` tells us which sample (across all chains and draws) to use for a full set of parameters.  **This is important:** If we sampled each parameter from different indices, we would effectively be sampling from the marginals, which is incorrect for generating predictive samples.\n",
    "\n",
    "### Generate the Posterior Predictive Samples from the Posterior Samples:\n",
    "\n",
    "For each posterior predictive sample:\n",
    "\n",
    " - Extract the corresponding set of `beta` coefficients and `sigma` value using the same index from `indices`\n",
    " \n",
    " - Compute the mean prediction: dot product of the `new_obs` vector and the sampled `beta`\n",
    " \n",
    " - Add random noise that is sampled from a Normal distribution with mean 0 and standard deviation `sigma[i]`\n",
    "\n",
    "We will then summarize our results manually using a `DataFrame`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26b8b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taste_score[0]</th>\n",
       "      <td>30.325</td>\n",
       "      <td>11.233</td>\n",
       "      <td>7.877</td>\n",
       "      <td>51.912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean      sd  hdi_2.5%  hdi_97.5%\n",
       "taste_score[0]  30.325  11.233     7.877     51.912"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ppd_samples = 1000\n",
    "\n",
    "#Create a stacked ArviZ/xarray Dataset object containing your posterior samples for beta and sigma. \n",
    "\n",
    "posterior_betas = trace.posterior.beta.stack(samples=('chain','draw')).values\n",
    "posterior_sigmas = trace.posterior.sigma.stack(samples=('chain','draw')).values\n",
    "\n",
    "_,num_samples = posterior_betas.shape\n",
    "\n",
    "\n",
    "#pre-defined list of indices\n",
    "indices = np.random.choice(a=num_samples,size=num_ppd_samples)\n",
    "\n",
    "#noise term:\n",
    "noise = np.random.normal(0,posterior_sigmas[indices])\n",
    "\n",
    "#posterior predictive samples:\n",
    "ppd = ((new_obs @ posterior_betas[:,indices]) + noise).flatten()\n",
    "\n",
    "#manually constructed summary:\n",
    "pd.DataFrame(   [dict(zip(\n",
    "                ['mean', 'sd', 'hdi_2.5%', 'hdi_97.5%'],\n",
    "                np.append([ppd.mean(), ppd.std()], az.hdi(ppd, hdi_prob=0.95)).round(3)))],\n",
    "                index=['taste_score[0]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c2e6c",
   "metadata": {},
   "source": [
    "The results are fairly close to the results from `az.summary(ppc.predictions, hdi_prob=0.95)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42361a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: Sun Mar 09 2025\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.7\n",
      "IPython version      : 8.29.0\n",
      "\n",
      "pytensor: 2.26.4\n",
      "\n",
      "pymc      : 5.19.1\n",
      "numpy     : 1.26.4\n",
      "arviz     : 0.20.0\n",
      "matplotlib: 3.9.2\n",
      "pandas    : 2.2.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -p pytensor"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py,md,Rmd"
  },
  "kernelspec": {
   "display_name": "pymc_macos15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}