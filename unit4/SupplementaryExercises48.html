

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Supplementary Exercises 4.8 &#8212; ISYE 6420 - BUGS to PyMC</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'unit4/SupplementaryExercises48';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Computation" href="../unit5/bayesian-computation.html" />
    <link rel="prev" title="Supplementary Exercises 4.3" href="SupplementaryExercises43.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo2.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo2.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit1/Unit1-about-this-course.html">1. About This Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit1/Unit1-topics.html">2. Topics Covered</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit1/Unit1-software.html">3. Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit1/Unit1-simple-regression.html">4. A Simple Regression*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit2/Unit2-history.html">1. History of Bayesian Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit2/Unit2-history2.html">2. Historic Overview Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit2/Unit2-bayesianvsclassical.html">3. Bayesian vs. Frequentist</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit2/Unit2-10flips.html">4. Ten Coin Flips*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit2/Unit2-fda.html">5. FDA Recommendations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-NecessaryProbability.html">1. A Review of Necessary Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-conditioning1.html">2. Conditioning, Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-conditioning2.html">3. Conditioning, Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-Bayes-theorem.html">4. Bayes’ Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-ManufacturingBayes.html">5. Manufacturing Bayes*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-TwoHeadedCoin.html">6. Two-headed Coin*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-bayesnetworks.html">7. Bayes Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-Alarm.html">8. Alarm*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/Unit3-Emily.html">9. Supplementary Problem: Emily’s Vacation*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit3/SupplementaryExercises35.html">10. Supplementary Exercises*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 4</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="distributions.html">Probability Distributions</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Unit4-basicdist.html">1. Basic Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-numericalchar.html">2. Numerical Characteristics of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-jointconditionals.html">3. Joint and Conditional Distributions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="bayestheorem.html">Bayes’ Theorem</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Unit4-ingredients.html">4. Ingredients for Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-conjugatefamilies.html">5. Conjugate Families</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-IQexample.html">6. Example: Jeremy’s IQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-BetaPriors.html">7. Ten Coin Flips Revisited: Beta Plots*</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-poissongamma.html">8. Poisson–Gamma Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="conjugatecases.html">Bayesian Inference in Conjugate Cases</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Unit4-estimation.html">9. Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-crediblesets.html">10. Credible Intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-GammaGamma.html">11. Gamma Gamma*</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-testing.html">12. Bayesian Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-prediction.html">13. Prediction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="priors.html">Priors</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Unit4-elicitation.html">14. Prior Elicitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-noninform.html">15. Non-informative Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-samplesize.html">16. Prior Sample Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-eBay.html">17. eBay Purchase Example*</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="empiricalbayes.html">Empirical Bayes</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Unit4-parametric.html">18. Parametric</a></li>
<li class="toctree-l2"><a class="reference internal" href="Unit4-nonpara.html">19. Non-parametric</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="moreexamples.html">More Examples</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Unit4-AlphaParticles.html">20. Counts of Alpha Particles*</a></li>
<li class="toctree-l2"><a class="reference internal" href="SupplementaryExercises43.html">Supplementary Exercises 4.3</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Supplementary Exercises 4.8</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit5/bayesian-computation.html">Bayesian Computation</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-numericalapproaches.html">1. Numerical Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-norcau1.html">2. Normal–Cauchy Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-Laplace.html">3. Laplace’s Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-demo-Laplaces-method.html">4. Laplace’s Method Demo*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-mcmc.html">5. Markov Chain Monte Carlo</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit5/metropolis-hastings.html">The Metropolis Algorithm</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-Metropolis.html">6. Metropolis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-Metropolis1.html">7. Metropolis: Normal-Cauchy*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-MetropolisHastings.html">8. Metropolis–Hastings: Weibull-Exponential Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-MetropolisHastings2.html">9. Weibull Lifetimes*</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit5/gibbs-sampling.html">Gibbs Sampling</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-gibbsintro.html">10. Introduction to Gibbs Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-GibbsSampler.html">11. Normal-Cauchy Gibbs Sampler*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-gibbs2.html">12. Conjugate Gamma-Poisson Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-PumpsMCMC.html">13. Pumps*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-gibbsex3math.html">14. Change Point Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-Gibbs3.html">15. Coal Mining Disasters in the UK*</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit5/other.html">Other Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-hamiltonian.html">16. Hamiltonian Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit5/Unit5-other-methods.html">17. Going Further</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../unit5/SupplementaryExercises54.html">Supplementary Exercises</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-probabilisticprogramming.html">1. Probabilistic Programming Languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-doodlebug.html">2. Creating a Graphical Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-demo-doodle.html">3. Joint Probability Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-moreaboutpymc.html">4. More About PyMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-loadingdata.html">5. Loading Data, Step Function, and Deterministic Variables*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-missingdata.html">6. Missing Data*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-htesting.html">7. Hypothesis Testing*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-prediction.html">8. Prediction*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-censoring.html">9. Type-1 Censoring*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit6/Unit6-customlikelihood.html">10. The Zero Trick and Custom Likelihoods*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit7/hierarchical.html">Hierarchical Models</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-introduction.html">1. Introduction to Hierarchical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-why.html">2. Reasons to Use Hierarchical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-priors.html">3. Priors with Structural Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-hiddenmixtures.html">4. Priors as Hidden Mixtures*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-metaanalysis.html">5. Meta-analysis via Hierarchical Models*</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit7/linearmodels.html">Linear Models</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-anova.html">6. Analysis of Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-coagulation.html">7. Coagulation*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-factorial.html">8. Factorial Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-demo-factorial.html">9. Simvastatin*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-regression.html">10. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-demo-regression.html">11. Brozek index prediction*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-multipleregression.html">12. Multiple Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-demo-multiple.html">13. Multiple Regression: Brozek Index Prediction*</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unit7/othermodels.html">Other Models</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-other.html">14. Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-arrhythmia.html">15. GLM Examples*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-multinomial.html">16. Multinomial Logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-demo-multinomial.html">17. Multinomial regression*</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-multilevel.html">18. Multilevel Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unit7/Unit7-demo-multilevel.html">19. Paraguay Vaccination Status*</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../unit7/SupplementaryExercises74.html">Supplementary Exercises</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-missing.html">1. Missing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-missrats.html">2. Rats Example with Missing Data*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-timetoevent.html">3. Time-to-event Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-tteex1.html">4. Time-to-event Models: Example 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-tteex2.html">5. Time-to-event Models: Example 2*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit8/Unit8-tte-gastric.html">6. Time-to-event Models: Gastric Cancer*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-fit-selection-diagnostics.html">1. Model Fit, Selection, and Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-dic.html">2. Deviance Information Criterion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-fitsel.html">3. Model Fit and Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-hald.html">4. Hald*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-cpo.html">5. Conditional Predictive Ordinate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-gessell.html">6. Gesell*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-cumulative.html">7. Using the Empirical CDF and the Probability Integral Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-cumulative2.html">8. Cumulative Example*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-ssvs.html">9. Stochastic Search Variable Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit9/Unit9-hald2.html">10. SSVS: Hald*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-lister.html">1. Lister*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-growth.html">2. Dental Development*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-disasters.html">3. Revisiting UK Coal Mining Disasters*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-rasch.html">4. Rasch*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-italywines123.html">5. Wine Classification*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-katla.html">6. Predicting Using Censored Data*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit10/Unit10-sunspots.html">7. Prediction of Time Series*</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Back Matter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../backmatter/bibliography.html">1. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backmatter/latex_reference.html">2. Latex Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/areding/6420-pymc/blob/main/unit4/SupplementaryExercises48.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/areding/6420-pymc" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/areding/6420-pymc/issues/new?title=Issue%20on%20page%20%2Funit4/SupplementaryExercises48.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/unit4/SupplementaryExercises48.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Supplementary Exercises 4.8</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mosaic-virus">2. Mosaic Virus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#figo">3. FIGO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histocompatibility">4. Histocompatibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-fire-in-potters-lab">5. Neurons Fire in Potter’s Lab</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elicit-inverse-gamma-prior">6. Elicit Inverse Gamma Prior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derive-jeffreys-priors-for-poisson-lambda-bernoulli-p-and-geometric-p">7. Derive Jeffreys’ Priors for Poisson <span class="math notranslate nohighlight">\(\lambda\)</span>, Bernoulli <span class="math notranslate nohighlight">\(p\)</span>, and Geometric <span class="math notranslate nohighlight">\(p\)</span>.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-scenarios-for-the-probability-of-success">8. Two Scenarios for the Probability of Success</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jeffreys-prior-for-normal-precision">9. Jeffreys’ Prior for Normal Precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derive-jeffreys-prior-for-maxwells-theta">10. Derive Jeffreys’ Prior for Maxwell’s <span class="math notranslate nohighlight">\(\theta\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-jeffreys-priors">11. “Quasi” Jeffreys’ Priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#haldane-prior-for-binomial-p">12. Haldane Prior for Binomial p</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eliciting-a-normal-prior">13. Eliciting a Normal Prior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jigsaw">14. Jigsaw</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jeremy-and-poisson">15. Jeremy and Poisson</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#npeb-for-p-in-the-geometric-distribution">16. NPEB for <em>p</em> in the Geometric Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lifetimes-and-predictive-distribution">17. Lifetimes and Predictive Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-likelihood-with-improper-priors">18. Normal Likelihood with Improper Priors</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="supplementary-exercises-4-8">
<h1>Supplementary Exercises 4.8<a class="headerlink" href="#supplementary-exercises-4-8" title="Permalink to this headline">#</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This page contains solutions! We recommend attempting each problem before peeking.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Question 1 was a duplicate of <a class="reference external" href="https://areding.github.io/6420-pymc/unit4/SupplementaryExercises43.html#counts-of-alpha">4.3 question 16</a>.</p>
</div>
<section id="mosaic-virus">
<h2>2. Mosaic Virus<a class="headerlink" href="#mosaic-virus" title="Permalink to this headline">#</a></h2>
<p>A single leaf is taken from each of 8 different tobacco plants. Each leaf is then divided in half, and given one of two preparations of mosaic virus. Researchers wanted to examine if there is a difference in the mean number of lesions from the two preparations. Here is the raw data:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccc}
\text{Plant} &amp; \text{Prep 1} &amp; \text{Prep 2} \\
1 &amp; 38 &amp; 29 \\
2 &amp; 40 &amp; 35 \\
3 &amp; 26 &amp; 31 \\
4 &amp; 33 &amp; 31 \\
5 &amp; 21 &amp; 14 \\
6 &amp; 27 &amp; 37 \\
7 &amp; 41 &amp; 22 \\
8 &amp; 36 &amp; 25 \\
\end{array}
\end{split}\]</div>
<p>Assume the normal distribution for the difference between the populations/samples. Using a PPL, find:</p>
<ol class="arabic simple">
<li><p>the 95% credible set for <span class="math notranslate nohighlight">\(\mu_1 - \mu_2\)</span>, and</p></li>
<li><p>posterior probability of hypothesis <span class="math notranslate nohighlight">\(H_1: \mu_1 - \mu_2 \geq 0\)</span>.</p></li>
</ol>
<p>Use noninformative priors.</p>
<p>Hint: Since this is a paired two sample problem, a single model should be placed on the difference.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>See the hidden cell below for the code. This question asks for a PPL solution, but keep in mind we won’t use those until after the midterm in the current class format. It is possible to do this with Unit 4 or Unit 5 techniques, but we haven’t written up a solution using those yet.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-left head"><p></p></th>
<th class="text-right head"><p>mean</p></th>
<th class="text-right head"><p>sd</p></th>
<th class="text-right head"><p>hdi_2.5%</p></th>
<th class="text-right head"><p>hdi_97.5%</p></th>
<th class="text-right head"><p>mcse_mean</p></th>
<th class="text-right head"><p>mcse_sd</p></th>
<th class="text-right head"><p>ess_bulk</p></th>
<th class="text-right head"><p>ess_tail</p></th>
<th class="text-right head"><p>r_hat</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>mean</p></td>
<td class="text-right"><p>4.767</p></td>
<td class="text-right"><p>3.812</p></td>
<td class="text-right"><p>-2.67</p></td>
<td class="text-right"><p>12.371</p></td>
<td class="text-right"><p>0.049</p></td>
<td class="text-right"><p>0.039</p></td>
<td class="text-right"><p>6515</p></td>
<td class="text-right"><p>5423</p></td>
<td class="text-right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>precision</p></td>
<td class="text-right"><p>0.012</p></td>
<td class="text-right"><p>0.006</p></td>
<td class="text-right"><p>0.002</p></td>
<td class="text-right"><p>0.024</p></td>
<td class="text-right"><p>0</p></td>
<td class="text-right"><p>0</p></td>
<td class="text-right"><p>6542</p></td>
<td class="text-right"><p>6356</p></td>
<td class="text-right"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>variance</p></td>
<td class="text-right"><p>117.474</p></td>
<td class="text-right"><p>106.899</p></td>
<td class="text-right"><p>24.761</p></td>
<td class="text-right"><p>271.664</p></td>
<td class="text-right"><p>1.389</p></td>
<td class="text-right"><p>0.982</p></td>
<td class="text-right"><p>6542</p></td>
<td class="text-right"><p>6356</p></td>
<td class="text-right"><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="n">prep1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">38</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">36</span><span class="p">])</span>
<span class="n">prep2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">29</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">25</span><span class="p">])</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">prep1</span> <span class="o">-</span> <span class="n">prep2</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">sigma2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span>

    <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;likelihood&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">diff</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">3000</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [precision, mean]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='16000' class='' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [16000/16000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 1 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_2.5%</th>
      <th>hdi_97.5%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>4.723</td>
      <td>3.771</td>
      <td>-2.838</td>
      <td>12.063</td>
      <td>0.048</td>
      <td>0.036</td>
      <td>6552.0</td>
      <td>5865.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.012</td>
      <td>0.006</td>
      <td>0.002</td>
      <td>0.025</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>7070.0</td>
      <td>7276.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>variance</th>
      <td>115.911</td>
      <td>90.591</td>
      <td>26.121</td>
      <td>271.055</td>
      <td>1.174</td>
      <td>0.830</td>
      <td>7070.0</td>
      <td>7276.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</details>
</div>
</section>
<section id="figo">
<h2>3. FIGO<a class="headerlink" href="#figo" title="Permalink to this headline">#</a></h2>
<p>Despite the excellent prognosis of FIGO stage I, type I endometrial cancers, a substantial number of patients experience recurrence and die from this disease. <span id="id1">Zeimet <em>et al.</em> [<a class="reference internal" href="../backmatter/bibliography.html#id46" title="Alain Gustave Zeimet, Daniel Reimer, Monica Huszar, Boris J. Winterhoff, U. S. Puistola, Samira Abdel Azim, Elisabeth Müller-Holzner, A. Ben‐arie, Léon C. van Kempen, Edgar Petru, Stephan Wenzel Jahn, Yvette P. Geels, Leon Fag Massuger, Frédéric Amant, Stephan Polterauer, Elisa Lappi‐Blanco, Johan Bulten, Alexandra Meuter, Staci Tanouye, Peter Oppelt, Monika Stroh-Weigert, Alexander Reinthaller, Andrea Mariani, Werner O. Hackl, Michael Netzer, Uwe Schirmer, Ignace Vergote, Peter Altevogt, Christian Marth, and Mina Fogel. L1cam in early-stage type i endometrial cancer: results of a large multicenter evaluation. Journal of the National Cancer Institute, 105 15:1142-50, 2013. URL: https://api.semanticscholar.org/CorpusID:23394570.">2013</a>]</span> conducted a retrospective multicenter cohort study to determine the expression of L1CAM by immunohistochemistry in 1021 endometrial cancer specimens with the goal of predicting clinical outcomes. Of the 1021 included cancers, 17.7% were rated L1CAM-positive. Of these L1CAM-positive cancers, 51.4% recurred during follow-up compared with 2.9% of L1CAM-negative cancers. Patients with L1CAM-positive cancers had poorer disease-free and overall survival.</p>
<p>It is stated that L1CAM has been the best-ever published prognostic factor in FIGO stage I, type I endometrial cancers and shows clear superiority over the standardly used multifactor risk score. L1CAM expression in type I cancers indicates the need for adjuvant treatment. This adhesion molecule might serve as a treatment target for the fully humanized anti-L1CAM antibody currently under development for clinical use.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>FIGO I/I Endometrial Cancer</p></th>
<th class="head"><p>Recurred</p></th>
<th class="head"><p>Did Not Recur</p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>L1CAM Positive</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>L1CAM Negative</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>1021</p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple">
<li><p>Using the information supplied, fill in the table (round the entries to the closest integer).</p></li>
<li><p>The estimators of the population Sensitivity and Specificity are simple relative frequencies (ratios): True Positives (TP)/Recurred and True Negatives (TN)/Not Recurred. Consider now a Bayesian version of this problem. Using WinBUGS, model TP and TN as Binomials, place priors on population Sensitivity (<span class="math notranslate nohighlight">\(p_1\)</span>) and Specificity (<span class="math notranslate nohighlight">\(p_2\)</span>) and find their Bayesian estimators. Explore the estimators for your favorite choice of priors on <span class="math notranslate nohighlight">\(p_1\)</span> and <span class="math notranslate nohighlight">\(p_2\)</span>: Jeffreys’, uniform (0, 1), flat on logit, etc.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Solution by Jason Naramore. This is another PPL example.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-left head"><p></p></th>
<th class="text-right head"><p>mean</p></th>
<th class="text-right head"><p>sd</p></th>
<th class="text-right head"><p>hdi_2.5%</p></th>
<th class="text-right head"><p>hdi_97.5%</p></th>
<th class="text-right head"><p>mcse_mean</p></th>
<th class="text-right head"><p>mcse_sd</p></th>
<th class="text-right head"><p>ess_bulk</p></th>
<th class="text-right head"><p>ess_tail</p></th>
<th class="text-right head"><p>r_hat</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>sensitivity</p></td>
<td class="text-right"><p>0.79</p></td>
<td class="text-right"><p>0.037</p></td>
<td class="text-right"><p>0.717</p></td>
<td class="text-right"><p>0.859</p></td>
<td class="text-right"><p>0</p></td>
<td class="text-right"><p>0</p></td>
<td class="text-right"><p>13585</p></td>
<td class="text-right"><p>11945</p></td>
<td class="text-right"><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="n">total</span> <span class="o">=</span> <span class="mi">1021</span>
<span class="n">totalpositive</span> <span class="o">=</span> <span class="mi">181</span>  <span class="c1"># rounded 0.177* 1021= 180.7170</span>
<span class="n">totalnegative</span> <span class="o">=</span> <span class="mi">840</span>  <span class="c1"># % as 1021- 181</span>
<span class="n">tp</span> <span class="o">=</span> <span class="mi">93</span>  <span class="c1"># true positiveas rounded181 *0.514 =93.0340</span>
<span class="n">fp</span> <span class="o">=</span> <span class="mi">88</span>  <span class="c1"># alse positives,as 181-93</span>
<span class="n">fn</span> <span class="o">=</span> <span class="mi">24</span>  <span class="c1"># false negativesas rounded840 *0.029=24.3600</span>
<span class="n">tn</span> <span class="o">=</span> <span class="mi">816</span>  <span class="c1"># truenegatives, as840-24</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>

    <span class="c1"># priors</span>
    <span class="n">prior_sensitivity</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;sensitivity&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prior_specificity</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;specificity&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Binomial Likelihoods</span>
    <span class="n">sensitivity_likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span>
        <span class="s2">&quot;sensitivity_likelihood&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">prior_sensitivity</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">tp</span>
    <span class="p">)</span>
    <span class="n">specificity_likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span>
        <span class="s2">&quot;specificity_likelihood&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">prior_specificity</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">tn</span>
    <span class="p">)</span>

    <span class="c1"># sampling</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sensitivity, specificity]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='24000' class='' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [24000/24000 00:01&lt;00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 2 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_2.5%</th>
      <th>hdi_97.5%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sensitivity</th>
      <td>0.790</td>
      <td>0.037</td>
      <td>0.718</td>
      <td>0.863</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>12138.0</td>
      <td>11296.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>specificity</th>
      <td>0.902</td>
      <td>0.010</td>
      <td>0.882</td>
      <td>0.920</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>14302.0</td>
      <td>12679.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</details>
</div>
</section>
<section id="histocompatibility">
<h2>4. Histocompatibility<a class="headerlink" href="#histocompatibility" title="Permalink to this headline">#</a></h2>
<p>A patient who is waiting for an organ transplant needs a histocompatible donor who matches the patient’s human leukocyte antigen (HLA) type.</p>
<p>For a given patient, the number of matching donors per 1000 National Blood Bank records is modeled as Poisson with unknown rate <span class="math notranslate nohighlight">\(\lambda\)</span>. If a randomly selected group of 1000 records showed exactly one match, estimate <span class="math notranslate nohighlight">\(\lambda\)</span> in Bayesian fashion.</p>
<p>For <span class="math notranslate nohighlight">\(\lambda\)</span>​ assume:</p>
<ol class="arabic simple">
<li><p>Gamma <span class="math notranslate nohighlight">\(\text{Ga}(\alpha=2, \beta=1)\)</span>​ prior;</p></li>
<li><p>flat prior <span class="math notranslate nohighlight">\(\lambda = 1\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>​;</p></li>
<li><p>invariance prior <span class="math notranslate nohighlight">\(\pi(\lambda) = \frac{1}{\lambda}\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>​;</p></li>
<li><p>Jeffreys prior <span class="math notranslate nohighlight">\(\pi(\lambda) = \sqrt{\frac{1}{\lambda}}\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Gamma <span class="math notranslate nohighlight">\(\text{Ga}(\alpha=2, \beta=1)\)</span>​ prior;</p></li>
</ol>
<p>Poisson PDF <span class="math notranslate nohighlight">\(\propto e^{-\lambda}\lambda^k\)</span>​​</p>
<p>To shake things up, let’s generalize to multiple independent datapoints, even though we only have a single datapoint equalling 1 for this problem.</p>
<p><span class="math notranslate nohighlight">\(\prod_{i=1}^{n} e^{-\lambda}\lambda^{k_i} = e^{- n\lambda}\lambda^{\sum_{i=1}^{n} k_i}\)</span>​</p>
<p>Gamma PDF <span class="math notranslate nohighlight">\(\propto x^{\alpha -1}e^{-\beta x}\)</span>​</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\pi(\lambda \mid k) &amp;\propto \left(e^{- n\lambda}\lambda^{\sum_{i=1}^{n} k_i}\right) \left(\lambda^{\alpha -1}e^{-\beta \lambda}\right) \\
&amp;\propto \lambda^{(\alpha -1) + \sum_{i=1}^{n} k_i}e^{-\beta \lambda - n\lambda} \\ 
&amp;\propto \lambda^{(\alpha + \sum_{i=1}^{n} k_i) - 1}e^{-(\beta + n)\lambda} \\
&amp;= Ga(\alpha + \sum_{i=1}^{n} k_i, \beta + n) 
\end{align*}\]</div>
<p>We recognize the <span class="math notranslate nohighlight">\(Ga(\alpha + \sum_{i=1}^{n} k_i, \beta + n)\)</span> posterior, which comes out to <span class="math notranslate nohighlight">\(Ga(3, 2)\)</span> in this case. Our Bayes estimate is then the mean of the posterior, which is <span class="math notranslate nohighlight">\(\frac{\alpha}{\beta} = \frac{3}{2}\)</span>.</p>
<ol class="arabic simple" start="2">
<li><p>flat prior <span class="math notranslate nohighlight">\(\lambda = 1\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>​;</p></li>
</ol>
<p>Then go on with the same procedure:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\pi(\lambda \mid k) &amp;\propto \left(e^{- n\lambda}\lambda^{\sum_{i=1}^{n} k_i}\right) \mathbf{1}(\lambda &gt; 0) \\
&amp;\propto \lambda^{\left(\sum_{i=1}^{n} k_i\right) + 1 - 1} e^{- n\lambda} \mathbf{1}(\lambda &gt; 0) \\
&amp;= Ga(1 + \sum_{i=1}^{n} k_i, n) 
\end{align*}\]</div>
<p>Which in our case would be <span class="math notranslate nohighlight">\(Ga(2, 1)\)</span> with a mean of 2.</p>
<ol class="arabic simple" start="3">
<li><p>invariance prior <span class="math notranslate nohighlight">\(\pi(\lambda) = \frac{1}{\lambda}\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>​;</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pi(\lambda \mid k) &amp; \propto \left(e^{- n\lambda}\lambda^{\sum_{i=1}^{n} k_i}\right) \frac{1}{\lambda}\mathbf{1}(\lambda &gt; 0) \\
&amp;\propto e^{-n\lambda} \lambda^{-1 + \sum_{i=1}^{n} k_i} \mathbf{1}(\lambda &gt; 0) \\
&amp; = Ga(\sum_{i=1}^{n} k_i, n) 
\end{align*}\]</div>
<p>We identify the <span class="math notranslate nohighlight">\(Ga(1, 1)\)</span> distribution, which has a mean of 1. Equivalently, the <span class="math notranslate nohighlight">\(Exp(1)\)</span> distribution.</p>
<ol class="arabic simple" start="4">
<li><p>Jeffreys prior <span class="math notranslate nohighlight">\(\pi(\lambda) = \sqrt{\frac{1}{\lambda}}\)</span>, for <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\pi(\lambda \mid k) &amp;\propto \left(e^{- n\lambda}\lambda^{\sum_{i=1}^{n} k_i}\right) \times \sqrt{\frac{1}{\lambda}}\mathbf{1}(\lambda &gt; 0) \\
&amp;\propto \lambda^{- 1/2 + \sum_{i=1}^{n} k_i} e^{-n\lambda} \mathbf{1}(\lambda &gt; 0) \\
&amp; \propto \lambda^{- 1 + \left(1/2 + \sum_{i=1}^{n} k_i\right)} e^{-n\lambda} \mathbf{1}(\lambda &gt; 0) \\
&amp;= Ga(1/2 + \sum_{i=1}^{n} k_i, n) 
\end{align*}\]</div>
<p>In our case the posterior is <span class="math notranslate nohighlight">\(Ga(3/2, 1)\)</span> with a mean of <span class="math notranslate nohighlight">\(3/2\)</span>.</p>
<p>Note that the priors in (b-d) are not proper densities (the integrals are not finite), however, the resulting posteriors are proper.</p>
</div>
</section>
<section id="neurons-fire-in-potters-lab">
<h2>5. Neurons Fire in Potter’s Lab<a class="headerlink" href="#neurons-fire-in-potters-lab" title="Permalink to this headline">#</a></h2>
<p>Data set consisting of 989 firing times in a cell culture of neurons, recorded time instances when a neuron sent a signal to another linked neuron (a spike). The cells from the cortex of an embryonic rat brain were cultured for 18 days on multielectrode arrays. The measurements were taken while the culture was stimulated at a rate of 1 Hz. From this data set, the counts of firings in consecutive time intervals of length 20 milliseconds were derived:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccccc}
20 &amp; 19 &amp; 26 &amp; 20 &amp; 24 \\
21 &amp; 24 &amp; 29 &amp; 21 &amp; 17 \\
23 &amp; 21 &amp; 19 &amp; 23 &amp; 17 \\
30 &amp; 20 &amp; 20 &amp; 18 &amp; 16 \\
14 &amp; 17 &amp; 15 &amp; 25 &amp; 21 \\
16 &amp; 14 &amp; 18 &amp; 22 &amp; 25 \\
17 &amp; 25 &amp; 24 &amp; 18 &amp; 13 \\
12 &amp; 19 &amp; 17 &amp; 19 &amp; 19 \\
19 &amp; 23 &amp; 17 &amp; 17 &amp; 21 \\
15 &amp; 19 &amp; 15 &amp; 23 &amp; 22 \\
\end{array}
\end{split}\]</div>
<p>It is believed that the counts are distributed as Poisson with an unknown parameter <span class="math notranslate nohighlight">\( \lambda \)</span>. An expert believes that the number of counts in the interval of 20 milliseconds should be about 15.</p>
<ol class="arabic simple">
<li><p>What is the likelihood function for these 50 observations?</p></li>
<li><p>Using the information the expert provided, elicit an appropriate Gamma prior. Is such a prior unique?</p></li>
<li><p>For the prior suggested in (2), find the Bayes’ estimator of <span class="math notranslate nohighlight">\( \lambda \)</span>. How does this estimator compare to the MLE?</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The likelihood is proportional to <span class="math notranslate nohighlight">\( \lambda^{\sum_{i=1}^{50} X_i} \exp\{-50\lambda\} \)</span>, where <span class="math notranslate nohighlight">\(\sum X_i = 989\)</span> is the sum of all counts (total number of firings). The <span class="math notranslate nohighlight">\(\sum_i X_i\)</span> is a sufficient statistic here and has a Poisson <span class="math notranslate nohighlight">\(\text{Poi}(n\lambda)\)</span> distribution.</p></li>
<li><p>A gamma prior with mean 15 is not unique; for any <span class="math notranslate nohighlight">\( x \)</span>, <span class="math notranslate nohighlight">\( \text{Ga}(15x, x) \)</span> is such a prior. However, the variances depend on <span class="math notranslate nohighlight">\( x \)</span>. For example, priors <span class="math notranslate nohighlight">\( \text{Ga}(150, 10) \)</span>, <span class="math notranslate nohighlight">\( \text{Ga}(15, 1) \)</span>, <span class="math notranslate nohighlight">\( \text{Ga}(1.5, 0.1) \)</span>, <span class="math notranslate nohighlight">\( \text{Ga}(0.15, 0.01) \)</span>, etc., have variances 1.5, 15, 150, 1500, etc. The variances indicate the degree of certainty of the expert that the prior mean is 15. Large variances correspond to non-informative choices. Since the sample variance of 50 observations is about 15, it is reasonable to take a prior with larger variance, say <span class="math notranslate nohighlight">\( \text{Ga}(3, 0.2) \)</span>.</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\( \lambda \mid \sum_i X_i \)</span> is gamma <span class="math notranslate nohighlight">\( \text{Ga} \left( \sum_i X_i + 3, n + 0.2 \right) \)</span>. The Bayes estimator for <span class="math notranslate nohighlight">\( \lambda \)</span> can be represented as <span class="math notranslate nohighlight">\( w \times \bar{X} + (1 - w) \times 15 \)</span>, where <span class="math notranslate nohighlight">\( w = \frac{n}{n + 0.2} \)</span>, emphasizing the fact that the posterior mean is a compromise between the MLE, <span class="math notranslate nohighlight">\( \bar{X} \)</span>, and the prior mean, 15.</p></li>
</ol>
</div>
</section>
<section id="elicit-inverse-gamma-prior">
<h2>6. Elicit Inverse Gamma Prior<a class="headerlink" href="#elicit-inverse-gamma-prior" title="Permalink to this headline">#</a></h2>
<p>Specify the inverse gamma prior</p>
<div class="math notranslate nohighlight">
\[
\pi(\theta) = \frac{\beta^\alpha \exp\{-\theta/\beta\}}{\Gamma(\alpha)\theta^{\alpha+1}}, \quad \theta \geq 0; \, \alpha, \beta &gt; 0
\]</div>
<p>if <span class="math notranslate nohighlight">\( E[\theta] = 2 \)</span> and <span class="math notranslate nohighlight">\( \text{Var}(\theta) = 12 \)</span> are elicited from the experts.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Show that the mean <span class="math notranslate nohighlight">\( \mu \)</span> and variance <span class="math notranslate nohighlight">\( \sigma^2 \)</span> of an inverse gamma prior <span class="math notranslate nohighlight">\( \text{IG}(\alpha, \beta) \)</span> are connected with <span class="math notranslate nohighlight">\( \alpha \)</span> and <span class="math notranslate nohighlight">\( \beta \)</span> as</p>
<div class="math notranslate nohighlight">
\[
\alpha = \frac{\mu^2}{\sigma^2} + 2, \quad \beta = \mu \left( \frac{\mu^2}{\sigma^2} + 1 \right)
\]</div>
<p>Result: <span class="math notranslate nohighlight">\( \alpha = \frac{7}{3}, \, \beta = \frac{8}{3} \)</span>.</p>
</div>
</section>
<section id="derive-jeffreys-priors-for-poisson-lambda-bernoulli-p-and-geometric-p">
<h2>7. Derive Jeffreys’ Priors for Poisson <span class="math notranslate nohighlight">\(\lambda\)</span>, Bernoulli <span class="math notranslate nohighlight">\(p\)</span>, and Geometric <span class="math notranslate nohighlight">\(p\)</span>.<a class="headerlink" href="#derive-jeffreys-priors-for-poisson-lambda-bernoulli-p-and-geometric-p" title="Permalink to this headline">#</a></h2>
<p>Recall that Jeffreys’ prior for parameter <span class="math notranslate nohighlight">\(\theta\)</span> in the likelihood <span class="math notranslate nohighlight">\(f(x | \theta)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\pi(\theta) \propto \left| \text{det}(I(\theta)) \right|^{1/2}\]</div>
<p>where, for univariate parameters,</p>
<div class="math notranslate nohighlight">
\[I(\theta) = E \left[ \left( \frac{d \log f(x | \theta)}{d\theta} \right)^2 \right] = -E \left[ \frac{d^2 \log f(x | \theta)}{d\theta^2} \right]\]</div>
<p>and expectation is taken with respect to the random variable <span class="math notranslate nohighlight">\(X \sim f(x | \theta)\)</span>.</p>
<ol class="arabic simple">
<li><p>Show that Jeffreys’ prior for Poisson distribution** <span class="math notranslate nohighlight">\(f(x | \lambda) = \frac{\lambda^x}{x!} e^{-\lambda}\)</span>, <span class="math notranslate nohighlight">\(\lambda \geq 0\)</span>, <strong>is</strong> <span class="math notranslate nohighlight">\(\pi(\lambda) = \sqrt{\frac{1}{\lambda}}\)</span>.</p></li>
<li><p>Show that Jeffreys’ prior for Bernoulli distribution** <span class="math notranslate nohighlight">\(f(x | p) = p^x (1 - p)^{1-x}\)</span>, <span class="math notranslate nohighlight">\(0 \leq p \leq 1\)</span>, <strong>is</strong> <span class="math notranslate nohighlight">\(\pi(p) \propto \frac{1}{\sqrt{p(1-p)}}\)</span>, which is the beta <span class="math notranslate nohighlight">\(\text{Be}(1/2, 1/2)\)</span> distribution (or Arcsin distribution).</p></li>
<li><p>Show that Jeffreys’ prior for Geometric distribution** <span class="math notranslate nohighlight">\(f(x | p) = (1 - p)^{x-1} p\)</span>, <span class="math notranslate nohighlight">\(x = 1, 2, \ldots\)</span> ; <span class="math notranslate nohighlight">\(0 \leq p \leq 1\)</span>, <strong>is</strong> <span class="math notranslate nohighlight">\(\pi(p) \propto \frac{1}{p \sqrt{1-p}}\)</span>.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p><strong>7. Derive Jeffreys’ Priors for Poisson</strong> <span class="math notranslate nohighlight">\(\lambda\)</span>, <strong>Bernoulli</strong> <span class="math notranslate nohighlight">\(p\)</span>, <strong>and Geometric</strong> <span class="math notranslate nohighlight">\(p\)</span>.</p>
<ol class="arabic simple">
<li></li>
</ol>
<p>For the Poisson distribution with likelihood function:
$<span class="math notranslate nohighlight">\(f(x | \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\)</span>$</p>
<p>First, we differentiate the log-likelihood with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{d\lambda} \log \left( \frac{\lambda^x e^{-\lambda}}{x!} \right) = \frac{d}{d\lambda} (x \log \lambda - \lambda)\]</div>
<p>Which gives:</p>
<div class="math notranslate nohighlight">
\[\frac{x}{\lambda} - 1\]</div>
<p>Now, the Fisher Information <span class="math notranslate nohighlight">\(I(\lambda)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[I(\lambda) = E\left[ \left( \frac{x}{\lambda} - 1 \right)^2 \right] = \frac{E[x^2]}{\lambda^2} - \frac{2E[x]}{\lambda} + 1\]</div>
<p>Given that <span class="math notranslate nohighlight">\(E[x^2] = Var(x) + (E[x])^2\)</span> and for a Poisson distribution, <span class="math notranslate nohighlight">\(E[x] = \lambda\)</span> and <span class="math notranslate nohighlight">\(Var(x) = \lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[E[x^2] = \lambda + \lambda^2\]</div>
<p>Substituting this in, we get:</p>
<div class="math notranslate nohighlight">
\[I(\lambda) = \frac{1}{\lambda}\]</div>
<p>Thus, the Jeffreys’ prior is:</p>
<div class="math notranslate nohighlight">
\[\pi(\lambda) \propto \sqrt{\frac{1}{\lambda}}\]</div>
<ol class="arabic simple" start="2">
<li></li>
</ol>
<p>For the Bernoulli distribution:
$<span class="math notranslate nohighlight">\(f(x | p) = p^x (1-p)^{1-x}\)</span>$</p>
<p>The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[L = x \log(p) + (1 - x) \log(1 - p)\]</div>
<p>Differentiating <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(p\)</span> we get:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial p} = \frac{x}{p} - \frac{1-x}{1-p}\]</div>
<p>And the second derivative is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 L}{\partial p^2} = -\frac{x}{p^2} - \frac{1-x}{(1-p)^2}\]</div>
<p>For a Bernoulli distribution, <span class="math notranslate nohighlight">\(E[x] = p\)</span>. The Fisher Information <span class="math notranslate nohighlight">\(I(p)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[I(p) = \frac{1}{p(1-p)}\]</div>
<p>So, the Jeffreys’ prior is:</p>
<div class="math notranslate nohighlight">
\[\pi(p) \propto \frac{1}{\sqrt{p(1-p)}}\]</div>
<ol class="arabic simple" start="3">
<li></li>
</ol>
<p>For the Geometric distribution:</p>
<div class="math notranslate nohighlight">
\[f(x | p) = (1-p)^{x-1} p\]</div>
<p>The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[L = (x-1) \log(1-p) + \log(p)\]</div>
<p>Differentiating <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial p} = \frac{1}{p} - \frac{x-1}{1-p}\]</div>
<p>And the second derivative is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 L}{\partial p^2} = -\frac{1}{p^2} - \frac{x-1}{(1-p)^2}\]</div>
<p>For a Geometric distribution, <span class="math notranslate nohighlight">\(E[x] = \frac{1}{p}\)</span>. The Fisher Information <span class="math notranslate nohighlight">\(I(p)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[I(p) = \frac{1}{p^2(1-p)}\]</div>
<p>So, the Jeffreys’ prior is:</p>
<div class="math notranslate nohighlight">
\[\pi(p) \propto \frac{1}{p \sqrt{1-p}}\]</div>
</div>
</section>
<section id="two-scenarios-for-the-probability-of-success">
<h2>8. Two Scenarios for the Probability of Success<a class="headerlink" href="#two-scenarios-for-the-probability-of-success" title="Permalink to this headline">#</a></h2>
<p>An experiment may lead to success with probability <span class="math notranslate nohighlight">\( p \)</span>, which is to be estimated. Two series of experiments were conducted:</p>
<ul class="simple">
<li><p>In the first scenario, the experiment is repeated independently 10 times, and the number of successes realized was 1.</p></li>
<li><p>In the second scenario, the experiment was repeated until success, and the number of repetitions was 10.</p></li>
</ul>
<ol class="arabic simple">
<li><p>The two likelihoods are Binomial and Geometric, and the moment matching estimate for the probability of success in both cases is <span class="math notranslate nohighlight">\( \hat{p} = 0.1 \)</span>. However, classical inference for the two cases (confidence intervals, testing, etc.) is different. Is there any difference in Bayesian inferences? Why yes or no?</p></li>
<li><p>For any of the two scenarios, find the Bayes estimator of <span class="math notranslate nohighlight">\( p \)</span> if the prior is <span class="math notranslate nohighlight">\( \pi(p) = \frac{1}{p\sqrt{1-p}} \)</span>.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>For the Binomial distribution, <span class="math notranslate nohighlight">\( E[X] = np = 10p \)</span>, and with <span class="math notranslate nohighlight">\( X = 1 \)</span>, leading to <span class="math notranslate nohighlight">\( \hat{p} = 0.1 \)</span>. For the Geometric distribution, <span class="math notranslate nohighlight">\( E[N] = 1/p \)</span> and <span class="math notranslate nohighlight">\( N = 10 \)</span>, also leading to <span class="math notranslate nohighlight">\( \hat{p} = 0.1 \)</span>. However, see Example 9.16 (page 413) in the Engineering Biostatistics textbook, known as the Savage Disparity. Since in both cases the likelihood is proportional to <span class="math notranslate nohighlight">\( p(1-p)^9 \)</span>, Bayesian inference coincides, and for a Bayesian, the scenario is irrelevant; all that matters is one success and 10 trials.</p></li>
<li><p>The posterior is proportional to <span class="math notranslate nohighlight">\( (1-p)^{17/2} \)</span>, which is beta <span class="math notranslate nohighlight">\( \text{Be}(1, 19/2) \)</span>. Thus, the Bayes’ estimator is <span class="math notranslate nohighlight">\( \hat{p}_B = \frac{2}{21} \)</span>.</p></li>
</ol>
</div>
</section>
<section id="jeffreys-prior-for-normal-precision">
<h2>9. Jeffreys’ Prior for Normal Precision<a class="headerlink" href="#jeffreys-prior-for-normal-precision" title="Permalink to this headline">#</a></h2>
<p>The Jeffreys’ prior on the normal scale <span class="math notranslate nohighlight">\( \sigma \)</span> is <span class="math notranslate nohighlight">\( \pi(\sigma) = \frac{1}{\sigma} \)</span>. Consider the precision parameter <span class="math notranslate nohighlight">\( \tau = \frac{1}{\sigma^2} \)</span>.</p>
<p>Using the invariance property, show that Jeffreys’ prior for <span class="math notranslate nohighlight">\( \tau \)</span> is <span class="math notranslate nohighlight">\( \pi(\tau) = \frac{1}{\tau} \)</span>.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>We know that Jeffreys’ prior for <span class="math notranslate nohighlight">\( \sigma \)</span> is <span class="math notranslate nohighlight">\( \pi(\sigma) = \frac{1}{\sigma} \)</span>.</p>
<p>The invariance property states that if <span class="math notranslate nohighlight">\( \tau = \tau(\sigma) \)</span>, then</p>
<div class="math notranslate nohighlight">
\[
I^{1/2}(\tau) = I^{1/2}(\sigma) \left| \frac{d\sigma}{d\tau} \right|
\]</div>
<p>Here, <span class="math notranslate nohighlight">\( \sigma = \sqrt{\frac{1}{\tau}} \)</span> and <span class="math notranslate nohighlight">\( \frac{d\sigma}{d\tau} = -\frac{1}{2} \tau^{-3/2} \)</span>.</p>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\pi(\tau) = \pi(\sigma) \left| \frac{d\sigma}{d\tau} \right| = \sqrt{\frac{1}{1/\tau}} \times \frac{1}{2} \tau^{-3/2} = \frac{1}{2\tau}
\]</div>
<p>Since the derived prior is improper, we can drop the constant 2 in the denominator and take</p>
<div class="math notranslate nohighlight">
\[
\pi(\tau) = \frac{1}{\tau}
\]</div>
<p>as Jeffreys’ prior for the precision parameter <span class="math notranslate nohighlight">\( \tau \)</span>.</p>
</div>
</section>
<section id="derive-jeffreys-prior-for-maxwells-theta">
<h2>10. Derive Jeffreys’ Prior for Maxwell’s <span class="math notranslate nohighlight">\(\theta\)</span><a class="headerlink" href="#derive-jeffreys-prior-for-maxwells-theta" title="Permalink to this headline">#</a></h2>
<ol class="arabic">
<li><p>Show that Jeffreys’ prior for Maxwell’s rate parameter <span class="math notranslate nohighlight">\( \theta \)</span> is proportional to <span class="math notranslate nohighlight">\( \frac{1}{\theta} \)</span>. Maxwell density is given by</p>
<div class="math notranslate nohighlight">
\[f(x \mid \theta) = \sqrt{\frac{2}{\pi}} \theta^{3/2} x^2 \exp\left( -\frac{1}{2} \theta x^2 \right), \quad x \geq 0, \, \theta &gt; 0\]</div>
</li>
<li><p>Show that the flat prior on <span class="math notranslate nohighlight">\( \log \theta \)</span> is equivalent to <span class="math notranslate nohighlight">\( \frac{1}{\theta} \)</span> prior on <span class="math notranslate nohighlight">\( \theta \)</span>.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The second derivative of the log-likelihood, when evaluated, is free of <span class="math notranslate nohighlight">\( x \)</span>, making the expectation straightforward. Given the prior <span class="math notranslate nohighlight">\( \pi(\theta) = \frac{1}{\theta} \)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\( \phi = \log \theta \)</span> have a flat prior. Then,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\pi(\theta) = \pi(\phi) \left| \frac{d\phi}{d\theta} \right| = 1 \times \frac{1}{\theta}
\]</div>
</div>
</section>
<section id="quasi-jeffreys-priors">
<h2>11. “Quasi” Jeffreys’ Priors<a class="headerlink" href="#quasi-jeffreys-priors" title="Permalink to this headline">#</a></h2>
<p>Jeffreys himself often recommended priors different from Jeffreys’ priors. For example, for Poisson rate <span class="math notranslate nohighlight">\( \lambda \)</span> he recommended <span class="math notranslate nohighlight">\( \pi(\lambda) \propto \frac{1}{\lambda} \)</span> instead of <span class="math notranslate nohighlight">\( \pi(\lambda) \propto \sqrt{\frac{1}{\lambda}} \)</span>.</p>
<p>For <span class="math notranslate nohighlight">\( (\mu, \sigma^2) \)</span>, Jeffreys recommended <span class="math notranslate nohighlight">\( \pi(\mu, \sigma^2) \propto \frac{1}{\sigma^2} \)</span>. This prior is obtained as the product of separate one-dimensional Jeffreys’ priors for <span class="math notranslate nohighlight">\( \mu \)</span> and <span class="math notranslate nohighlight">\( \sigma^2 \)</span>.</p>
<p>Show that the simultaneous Jeffreys’ prior for the two-dimensional parameter <span class="math notranslate nohighlight">\( (\mu, \sigma^2) \)</span> is <span class="math notranslate nohighlight">\( \pi(\mu, \sigma^2) \propto \frac{1}{\sigma^3} \)</span>.</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Denote <span class="math notranslate nohighlight">\( \phi = \sigma^2 \)</span>. Then the normal likelihood is</p>
<div class="math notranslate nohighlight">
\[
L(\mu, \phi) = \frac{1}{\sqrt{2\pi\phi}} \exp\left( -\frac{(x - \mu)^2}{2\phi} \right)
\]</div>
<p>and the log likelihood is</p>
<div class="math notranslate nohighlight">
\[
\ell(\mu, \phi) = \text{const} - \frac{1}{2} \log \phi - \frac{(x - \mu)^2}{2\phi}
\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \ell}{\partial \mu} = \frac{x - \mu}{\phi}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \ell}{\partial \phi} = -\frac{1}{2\phi} + \frac{(x - \mu)^2}{2\phi^2}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 \ell}{\partial \mu^2} = -\frac{1}{\phi}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 \ell}{\partial \mu \partial \phi} = -\frac{x - \mu}{\phi^2}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 \ell}{\partial \phi \partial \mu} = \frac{\partial^2 \ell}{\partial \mu \partial \phi}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 \ell}{\partial \phi^2} = \frac{1}{2\phi^2} - \frac{(x - \mu)^2}{\phi^3}
\]</div>
<p>The Fisher Information matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
I = -E \left[
\begin{array}{cc}
-\frac{1}{\phi} &amp; -\frac{x - \mu}{\phi^2} \\
-\frac{x - \mu}{\phi^2} &amp; \frac{1}{2\phi^2} - \frac{(x - \mu)^2}{\phi^3}
\end{array}
\right] =
\begin{pmatrix}
\frac{1}{\phi} &amp; 0 \\
0 &amp; \frac{1}{2\phi^2}
\end{pmatrix}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\det(I) = \frac{1}{2\phi^3}
\]</div>
<p>Jeffreys’ prior is proportional to <span class="math notranslate nohighlight">\( |\det(I)|^{1/2} \)</span>, so</p>
<div class="math notranslate nohighlight">
\[
\pi(\mu, \phi) \propto \frac{1}{\phi^{3/2}} \propto \frac{1}{\sigma}
\]</div>
</div>
</section>
<section id="haldane-prior-for-binomial-p">
<h2>12. Haldane Prior for Binomial p<a class="headerlink" href="#haldane-prior-for-binomial-p" title="Permalink to this headline">#</a></h2>
<p><span id="id2">Haldane [<a class="reference internal" href="../backmatter/bibliography.html#id48" title="J. B. S. Haldane. A note on inverse probability. Mathematical Proceedings of the Cambridge Philosophical Society, 28(1):55–61, 1932. doi:10.1017/S0305004100010495.">1932</a>]</span> suggested a fully noninformative prior for binomial <span class="math notranslate nohighlight">\( p \)</span> as <span class="math notranslate nohighlight">\( \pi(p) \propto \frac{1}{p(1-p)} \)</span> [beta <span class="math notranslate nohighlight">\(\text{Be}(0, 0)\)</span> distribution].</p>
<ol class="arabic simple">
<li><p>Show that Haldane prior is equivalent to a flat prior on <span class="math notranslate nohighlight">\(\text{logit}(p)\)</span>.</p></li>
<li><p>Suppose <span class="math notranslate nohighlight">\( X \sim \text{Bin}(n, p) \)</span> is observed. What is the posterior? What is the Bayes estimator of <span class="math notranslate nohighlight">\( p \)</span>?</p></li>
<li><p>What is the predictive distribution for a single future Bernoulli <span class="math notranslate nohighlight">\( Y \)</span>? What is the prediction for <span class="math notranslate nohighlight">\( Y \)</span>?</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While tracking down the Haldane citation, I found <a class="reference external" href="https://arxiv.org/abs/1511.08180">this paper</a> (<span id="id3">Etz and Wagenmakers [<a class="reference internal" href="../backmatter/bibliography.html#id47" title="Alexander Etz and Eric-Jan Wagenmakers. J. b. s. haldane's contribution to the bayes factor hypothesis test. 2016. URL: https://arxiv.org/abs/1511.08180, arXiv:1511.08180.">2016</a>]</span>) on some Bayesian history relating to Jeffreys and Haldane and <a class="reference external" href="https://xianblog.wordpress.com/2015/11/27/origin-of-the-bayes-factor/">this blog post commenting on it</a> by one of my favorite Bayesian writers, Christian P. Robert.</p>
<p>Just leaving these links here if anyone is interested!</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Let <span class="math notranslate nohighlight">\( \psi \)</span> be the logit of <span class="math notranslate nohighlight">\( p \)</span>, i.e.,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\psi = \log\left(\frac{p}{1 - p}\right),
\]</div>
<p>and assume that <span class="math notranslate nohighlight">\( \psi \)</span> is given a flat prior, <span class="math notranslate nohighlight">\( \psi \sim 1 \)</span>. Then the prior on <span class="math notranslate nohighlight">\( p \)</span> is</p>
<div class="math notranslate nohighlight">
\[
\pi(p) = \pi(\psi) \left| \frac{d\psi(p)}{dp} \right| = 1 \times \left| \frac{1}{p/(1 - p)} \cdot \frac{(1 - p) - p}{(1 - p)^2} \right| = \frac{1}{p(1 - p)}.
\]</div>
<ol class="arabic simple" start="2">
<li><p>The posterior is beta <span class="math notranslate nohighlight">\( \text{Be}(x, n - x) \)</span> and the Bayes estimator of <span class="math notranslate nohighlight">\( p \)</span> is <span class="math notranslate nohighlight">\( \frac{x}{n} \)</span>, which coincides with the frequentist’s <span class="math notranslate nohighlight">\( \hat{p} \)</span>.</p></li>
</ol>
<p>© The predictive distribution for a single future Bernoulli <span class="math notranslate nohighlight">\( y \)</span> is</p>
<div class="math notranslate nohighlight">
\[
f(y \mid x) = \frac{B(x + y, n + 1 - x - y)}{B(x, n - x)}.
\]</div>
<p>Here, using <span class="math notranslate nohighlight">\( B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)} \)</span> and <span class="math notranslate nohighlight">\( \Gamma(a + 1) = a\Gamma(a) \)</span>, we can show</p>
<div class="math notranslate nohighlight">
\[
f(0 \mid x) + f(1 \mid x) = \frac{\Gamma(x)\Gamma(n + 1 - x)}{\Gamma(n + 1)} + \frac{\Gamma(x + 1)\Gamma(n - x)}{\Gamma(n + 1)},
\]</div>
<div class="math notranslate nohighlight">
\[
= \frac{\Gamma(x)(n - x)\Gamma(n - x)}{n\Gamma(n)} + \frac{x\Gamma(x)\Gamma(n - x)}{n\Gamma(n)} = \frac{n - x}{n} + \frac{x}{n} = 1.
\]</div>
<p>Thus, the distribution of future observation <span class="math notranslate nohighlight">\( y \)</span> given <span class="math notranslate nohighlight">\( x \)</span> successes in <span class="math notranslate nohighlight">\( n \)</span> trials is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y \mid x \quad \begin{array}{cc}
0 &amp; 1 \\
\frac{n - x}{n} &amp; \frac{x}{n}
\end{array}
\end{split}\]</div>
<p>Note that the prediction for future <span class="math notranslate nohighlight">\( y \)</span> is the mean of the posterior predictive distribution, which is <span class="math notranslate nohighlight">\( \frac{x}{n} \)</span>. The same result is obtained when <span class="math notranslate nohighlight">\( E[y] = p \)</span> is integrated with respect to the posterior <span class="math notranslate nohighlight">\( \text{Be}(x, n - x) \)</span>. Check this!</p>
</div>
</section>
<section id="eliciting-a-normal-prior">
<h2>13. Eliciting a Normal Prior<a class="headerlink" href="#eliciting-a-normal-prior" title="Permalink to this headline">#</a></h2>
<p>We are eliciting a normal prior <span class="math notranslate nohighlight">\( N(\mu, \sigma^2) \)</span> from an expert who can specify percentiles. If the 20th and 70th percentiles are specified as 2.7 and 4.8, respectively, how should <span class="math notranslate nohighlight">\( \mu \)</span> and <span class="math notranslate nohighlight">\( \sigma \)</span> be elicited?</p>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>If <span class="math notranslate nohighlight">\( x_p \)</span> is the <span class="math notranslate nohighlight">\( p \)</span>th quantile (100% <span class="math notranslate nohighlight">\( p \)</span>th percentile), then <span class="math notranslate nohighlight">\(x_p = \mu + z_p \sigma\)</span>. A system of two equations with two unknowns is formed with <span class="math notranslate nohighlight">\(z_p\)</span>.</p>
<p>The solution is <span class="math notranslate nohighlight">\(\mu = 3.99382 \approx 4\)</span>, <span class="math notranslate nohighlight">\(\sigma = 1.53734\)</span>.</p>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">z_20</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.20</span><span class="p">)</span>
<span class="n">z_70</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.70</span><span class="p">)</span>

<span class="n">z_20</span><span class="p">,</span> <span class="n">z_70</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.8416212335729142, 0.5244005127080407)
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="jigsaw">
<h2>14. Jigsaw<a class="headerlink" href="#jigsaw" title="Permalink to this headline">#</a></h2>
<p>An experiment with a sample of 18 nursery-school children involved the elapsed time required to put together a small jigsaw puzzle. The times were:</p>
<p>\begin{array}{cccccc}
3.1 &amp; 3.2 &amp; 3.4 &amp; 3.6 &amp; 3.7 &amp; 4.2 \
4.3 &amp; 4.5 &amp; 4.7 &amp; 5.2 &amp; 5.6 &amp; 6.0 \
6.1 &amp; 6.6 &amp; 7.3 &amp; 8.2 &amp; 10.8 &amp; 13.6 \
\end{array}</p>
<p>Assume that data are coming from normal <span class="math notranslate nohighlight">\( N(\mu, \sigma^2) \)</span> with <span class="math notranslate nohighlight">\( \sigma^2 = 8 \)</span>. For parameter <span class="math notranslate nohighlight">\( \mu \)</span>, set a normal prior with mean 5 and variance 6.</p>
<ol class="arabic simple">
<li><p>Find the Bayes estimator and 95% credible set for the population mean <span class="math notranslate nohighlight">\( \mu \)</span>.</p></li>
<li><p>Find the posterior probability of hypothesis <span class="math notranslate nohighlight">\( H_0: \mu \leq 5 \)</span>.</p></li>
<li><p>What is your prediction for a single future observation?</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Find the Bayes estimator and 95% credible set for population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
</ol>
<p>We can define our model like this:</p>
<div class="amsmath math notranslate nohighlight" id="equation-79a2647a-c898-4e4d-bd49-f108030bab60">
<span class="eqno">(1)<a class="headerlink" href="#equation-79a2647a-c898-4e4d-bd49-f108030bab60" title="Permalink to this equation">#</a></span>\[\begin{align}
x_i|\mu &amp;\sim N(\mu, 8) \\
\mu &amp; \sim N(5, 6)
\end{align}\]</div>
<p>This is the Normal-Normal conjugate pair for a fixed variance and random mean, with <span class="math notranslate nohighlight">\(n=18\)</span> and <span class="math notranslate nohighlight">\(\bar{X} \approx 5.78333\)</span></p>
<p>Our posterior is then:</p>
<div class="amsmath math notranslate nohighlight" id="equation-84cbc147-c76e-4847-856f-4ebd01ed8afa">
<span class="eqno">(2)<a class="headerlink" href="#equation-84cbc147-c76e-4847-856f-4ebd01ed8afa" title="Permalink to this equation">#</a></span>\[\begin{align}
\pi(\theta|x) &amp;\sim N\left(\frac{\tau^2}{\tau^2 +\sigma^2/n}\bar{X} + \frac{\sigma^2/n}{\tau^2 + \sigma^2/n}\mu_0, \frac{\tau^2\sigma^2/n}{\tau^2+ \sigma^2/n}\right) \\
&amp; \sim N(\frac{6}{6 + 8/18}\bar{X} + \frac{8/18}{6 + 8/18}(5), \frac{6(8/18)}{6+8/18}) \\
&amp; \sim N(5.72931, 0.41379)
\end{align}\]</div>
<p>Our Bayes estimator will be the posterior mean, 5.72931.</p>
<p>The 95% equitailed credible set: (4.4685, 6.9901). HPD set will be the same since this is a symmetrical posterior.</p>
<ol class="arabic simple" start="2">
<li><p>Find the posterior probability of hypothesis <span class="math notranslate nohighlight">\(H_0 : \mu \leq 5\)</span>.</p></li>
</ol>
<p>Use the posterior cdf (see code below) with a result of 0.1284.</p>
<ol class="arabic simple" start="3">
<li><p>What is your prediction for a single future observation?</p></li>
</ol>
<p>A single prediction in this case will be equal to the posterior mean. But the important thing about</p>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># part 1</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">ss</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mf">5.72931</span>
<span class="n">var</span> <span class="o">=</span> <span class="mf">0.41379</span>

<span class="n">post</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">var</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">round</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4.4685, 6.9901)
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># part 1 HPD</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fsolve</span>


<span class="k">def</span> <span class="nf">conditions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">lwr</span><span class="p">,</span> <span class="n">upr</span> <span class="o">=</span> <span class="n">x</span>

    <span class="n">cond_1</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">upr</span><span class="p">)</span> <span class="o">-</span> <span class="n">post</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">lwr</span><span class="p">)</span>
    <span class="n">cond_2</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">upr</span><span class="p">)</span> <span class="o">-</span> <span class="n">post</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">lwr</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cond_1</span><span class="p">,</span> <span class="n">cond_2</span>


<span class="n">fsolve</span><span class="p">(</span><span class="n">conditions</span><span class="p">,</span> <span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">post</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.46853355, 6.99008645])
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">round</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># post probability, part 2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1284
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pymc solution for Q14</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">mu_prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">6</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;lik&quot;</span><span class="p">,</span> <span class="n">mu_prior</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">8</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">extend_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="o">.</span><span class="n">to_array</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;stats&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='24000' class='' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [24000/24000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 1 seconds.
Sampling: [lik]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='20000' class='' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [20000/20000 00:00&lt;00:00]
    </div>
    </div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;xarray.DataArray ()&gt; Size: 8B
array(5.73871452)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_2.5%</th>
      <th>hdi_97.5%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mu</th>
      <td>5.727</td>
      <td>0.651</td>
      <td>4.451</td>
      <td>6.968</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</details>
</div>
</section>
<section id="jeremy-and-poisson">
<h2>15. Jeremy and Poisson<a class="headerlink" href="#jeremy-and-poisson" title="Permalink to this headline">#</a></h2>
<p>Jeremy believes that the normal model on his IQ test scores is not appropriate. After all, the scores are reported as integers. So he proposes a Poisson model; the scores to be modeled as Poisson:</p>
<div class="math notranslate nohighlight">
\[y \sim \text{Poisson}(\theta)\]</div>
<p>An expert versed in GT student’s intellectual abilities is asked to elicit a prior on <span class="math notranslate nohighlight">\(\theta\)</span>. The expert elicits a gamma prior:</p>
<div class="math notranslate nohighlight">
\[\theta \sim \text{Gamma}(30, 0.25)\]</div>
<p>Jeremy gets the test and scores <span class="math notranslate nohighlight">\(y = 98\)</span>.</p>
<ol class="arabic simple">
<li><p>What is the Bayes estimator of <span class="math notranslate nohighlight">\(\theta\)</span>? Find this estimator exactly.</p></li>
<li><p>Using a PPL, confirm that simulations agree with the theoretical result in (a).</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Use the conjugate table to find that <span class="math notranslate nohighlight">\(\theta \mid y \sim \text{Gamma}(128, \frac{5}{4})\)</span></p>
<p>The posterior mean is <span class="math notranslate nohighlight">\(128 \cdot \frac{4}{5} = 102.4\)</span> and variance <span class="math notranslate nohighlight">\(128 \cdot \frac{16}{25} = 81.92\)</span>. The posterior standard deviation is <span class="math notranslate nohighlight">\(9.0510\)</span>.</p>
<p>See the PyMC code below for part 2.</p>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pymc solution for Q15</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">theta_prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>

    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;lik&quot;</span><span class="p">,</span> <span class="n">theta_prior</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="p">[</span><span class="mi">98</span><span class="p">])</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">4000</span><span class="p">)</span>

<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;stats&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [theta]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='20000' class='' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [20000/20000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 4_000 draw iterations (4_000 + 16_000 draws total) took 1 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_2.5%</th>
      <th>hdi_97.5%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>theta</th>
      <td>102.301</td>
      <td>9.06</td>
      <td>85.352</td>
      <td>120.737</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</details>
</div>
</section>
<section id="npeb-for-p-in-the-geometric-distribution">
<h2>16. NPEB for <em>p</em> in the Geometric Distribution<a class="headerlink" href="#npeb-for-p-in-the-geometric-distribution" title="Permalink to this headline">#</a></h2>
<p>A geometric random variable <span class="math notranslate nohighlight">\(X\)</span> counts the number of failures before the first success, when the probability of success is <span class="math notranslate nohighlight">\(p\)</span> (and a failure <span class="math notranslate nohighlight">\(1 - p\)</span>). The PDF of <span class="math notranslate nohighlight">\(X\)</span> is:</p>
<p><span class="math notranslate nohighlight">\(P(X = x) = (1 - p)^x \times p, \quad x = 0, 1, 2, \ldots; \quad 0 \leq p \leq 1\)</span></p>
<p>We simulated a sample of size 2400 from a geometric distribution with a probability of success 0.32. The following (summarized) sample was obtained:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span></p></th>
<th class="head"><p>Frequency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>758</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>527</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>379</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>229</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>162</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>121</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>79</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>56</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>15</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>21+</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p><strong>Total</strong></p></td>
<td><p><strong>2400</strong></p></td>
</tr>
</tbody>
</table>
<ol class="arabic simple">
<li><p>Develop a Nonparametric Empirical Bayes Estimator if the prior on <span class="math notranslate nohighlight">\(p\)</span> is <span class="math notranslate nohighlight">\(g(p)\)</span>, <span class="math notranslate nohighlight">\(0 \leq p \leq 1\)</span>.</p></li>
<li><p>Compute the empirical Bayes estimator developed in 1. on the simulated sample for different values of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Solution for 1:
The likelihood and prior are:</p>
<div class="math notranslate nohighlight">
\[f(x \mid p) = (1 - p)^x p, \quad x = 0, 1, 2, \ldots; \quad p \sim g(p), \quad 0 \leq p \leq 1\]</div>
<p>leading to the marginal for <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[m(x) = \int_0^1 f(x \mid p) \, dG(p) = \int_0^1 (1 - p)^x p g(p) \, dp\]</div>
<p>The posterior mean is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
E(p \mid x) &amp;= \frac{1}{m(x)} \int_0^1 p (1 - p)^x p g(p) \, dp \\
&amp;= \frac{1}{m(x)} \int_0^1 [1 - (1 - p)] (1 - p)^x p g(p) \, dp \\
&amp;= \frac{1}{m(x)} \left[ m(x) - \int_0^1 (1 - p)^{x+1} p g(p) \, dp \right] \\
&amp;= 1 - \frac{m(x+1)}{m(x)}
\end{align*}\]</div>
<p>An automatic estimator for <span class="math notranslate nohighlight">\(m(x)\)</span> is:</p>
<p><span class="math notranslate nohighlight">\(\hat{m}(x) = \frac{\# \text{ of observations } = x}{\text{Total } \# \text{ of observations}}\)</span></p>
<p>This leads to:</p>
<div class="math notranslate nohighlight">
\[p^* = 1 - \frac{\# \text{ of observations } = x + 1}{\# \text{ of observations } = x}\]</div>
<div class="math notranslate nohighlight">
\[\hat{p} = \min \left\{1, \max \{0, p^*\} \right\}\]</div>
<p>In this case <span class="math notranslate nohighlight">\(\hat{p}\)</span> is free of the prior distribution <span class="math notranslate nohighlight">\(g\)</span> (although the marginal depends on <span class="math notranslate nohighlight">\(g\)</span>).</p>
<p>Solution for 2:
For the simulated data, the estimators (at particular values of <span class="math notranslate nohighlight">\(x\)</span>) are given in the following table:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span></p></th>
<th class="head"><p>Frequency</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat{p}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>758</p></td>
<td><p>0.3047</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>527</p></td>
<td><p>0.2808</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>379</p></td>
<td><p>0.3958</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>229</p></td>
<td><p>0.2926</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>162</p></td>
<td><p>0.2531</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>121</p></td>
<td><p>0.3471</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>79</p></td>
<td><p>0.2911</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>56</p></td>
<td><p>0.4643</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>30</p></td>
<td><p>0.3333</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>20</p></td>
<td><p>0.2500</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>15</p></td>
<td><p>0.6000</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>6</p></td>
<td><p>0.0000</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>6</p></td>
<td><p>0.3333</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>4</p></td>
<td><p>0.7500</p></td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p>1</p></td>
<td><p>1.0000</p></td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><p>0</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><p>4</p></td>
<td><p>0.7500</p></td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>21+</p></td>
<td><p>0</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-even"><td><p><strong>Total</strong></p></td>
<td><p><strong>2400</strong></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>Note that for <span class="math notranslate nohighlight">\(x \geq 10\)</span> the NPEB estimators become unreliable due to low frequency counts.</p>
</div>
</section>
<section id="lifetimes-and-predictive-distribution">
<h2>17. Lifetimes and Predictive Distribution<a class="headerlink" href="#lifetimes-and-predictive-distribution" title="Permalink to this headline">#</a></h2>
<p>Suppose that <span class="math notranslate nohighlight">\(T_1, \ldots, T_n\)</span> are exponential <span class="math notranslate nohighlight">\(\text{Exp}(\theta)\)</span> lifetimes, where <span class="math notranslate nohighlight">\(\theta\)</span> is the rate parameter. Let the prior on <span class="math notranslate nohighlight">\(\theta\)</span> be exponential <span class="math notranslate nohighlight">\(\text{Exp}(\tau)\)</span>, where <span class="math notranslate nohighlight">\(\tau\)</span> is also a rate parameter.</p>
<p>Denote with <span class="math notranslate nohighlight">\(T\)</span> the total observed lifetime <span class="math notranslate nohighlight">\(\sum_{i=1}^n T_i\)</span>. Then, <span class="math notranslate nohighlight">\(T\)</span> is gamma <span class="math notranslate nohighlight">\(\text{Gamma}(n, \theta)\)</span> distributed. Show:</p>
<ol class="arabic simple">
<li><p>Marginal (prior predictive) for <span class="math notranslate nohighlight">\(T\)</span> is</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
m_T(t) = \frac{n\tau t^{n-1}}{(\tau+t)^{n+1}}, \quad t &gt; 0
\]</div>
<ol class="arabic simple" start="2">
<li><p>Posterior for <span class="math notranslate nohighlight">\(\theta\)</span> given <span class="math notranslate nohighlight">\(T = t\)</span> is <span class="math notranslate nohighlight">\(\text{Gamma}(n + 1, \tau + t)\)</span>.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\pi(\theta \mid y) = \frac{\theta^n (\tau + t)^{n+1}}{\Gamma(n + 1)} \exp\{- (\tau + t) \theta\}
\]</div>
<ol class="arabic simple" start="3">
<li><p>Posterior predictive distribution for a new <span class="math notranslate nohighlight">\(T^*\)</span>, given <span class="math notranslate nohighlight">\(T = t\)</span> is</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
f(t^* \mid t) = \int_0^\infty \theta \exp \{- \theta t^* \} \pi(\theta \mid t) \, d\theta = \frac{(n + 1)(\tau + t)^{n+1}}{(\tau + t + t^*)^{n+2}}
\]</div>
<ol class="arabic simple" start="4">
<li><p>Expected value (with respect to the posterior predictive distribution) of <span class="math notranslate nohighlight">\(T^*\)</span> (that is, the prediction for a new <span class="math notranslate nohighlight">\(T^*\)</span>) is</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
E(T^* \mid T = t) = \frac{\tau + t}{n}
\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
m_T (t) &amp;= \int_{\Theta} f(T \mid \theta) \pi(\theta) \, d\theta \\
\end{align*}\]</div>
<p>Substituting for the given gamma and exponential distributions:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
m_T (t) &amp;= \int_0^\infty \frac{\theta^n}{\Gamma(n)} t^{n-1} e^{-\theta t} \tau e^{-\tau \theta} \, d\theta \\
&amp;= \frac{\tau t^{n-1}}{\Gamma(n)} \int_0^\infty \theta^n e^{-\theta (t+\tau)} \, d\theta
\end{align*}\]</div>
<p>Let <span class="math notranslate nohighlight">\(u = \theta(\tau + t)\)</span>, <span class="math notranslate nohighlight">\(du = d\theta (\tau + t) \rightarrow d\theta = \frac{du}{\tau + t}\)</span>. The bounds of integration remain the same in this case. Substituting:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\tau t^{n-1}}{\Gamma(n)} \int_0^\infty \left( \frac{u}{\tau + t} \right)^n e^{-u} \frac{du}{\tau + t} &amp;= \frac{\tau t^{n-1}}{\Gamma(n)(\tau + t)^{n+1}} \int_0^\infty u^n e^{-u} \, du
\end{align*}\]</div>
<p>The integral by definition is <span class="math notranslate nohighlight">\(\Gamma(n + 1)\)</span>, substituting:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\tau t^{n-1}}{(\tau + t)^{n+1}} \frac{\Gamma(n + 1)}{\Gamma(n)} &amp;= \frac{\tau t^{n-1}}{(\tau + t)^{n+1}} \frac{n!}{(n - 1)!} \\
&amp;= \frac{\tau t^{n-1}}{(\tau + t)^{n+1}} \frac{n \cdot (n - 1)!}{(n - 1)!}
\end{align*}\]</div>
<ol class="arabic simple" start="2">
<li><p>We just showed that the product of the marginal and prior is:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\tau t^{n-1}}{\Gamma(n)} \theta^n e^{-\theta (t + \tau)}
\end{align*}\]</div>
<p>This is proportional to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\theta^n e^{-\theta (t + \tau)}
\end{align*}\]</div>
<p>Which is proportional to <span class="math notranslate nohighlight">\(\text{Gamma}(n+1, \tau + t)\)</span></p>
<ol class="arabic simple" start="3">
<li><p>We are given the definition of posterior predictive with the distributions substituted in so all we have to do is evaluate the integral, it is the same procedure as (a) but now <span class="math notranslate nohighlight">\(u = \theta(\tau + t + t^*)\)</span></p></li>
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
E[T^* \mid T = t] &amp;\equiv \int t^* f(t^* \mid t) \, dt^* = \int_0^\infty t^* \frac{(n + 1)(\tau + t)^{n+1}}{(\tau + t + t^*)^{n+2}} \, dt^*
\end{align*}\]</div>
<p>substitute <span class="math notranslate nohighlight">\(\tau + t + t^* = u, \, dt^* = du\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
= (n + 1)(\tau + t)^{n+1} \int_{t+\tau}^\infty \frac{u - t - \tau}{u^{n+2}} \, du &amp;= (n + 1)(\tau + t)^{n+1} \int_{t+\tau}^\infty \left( \frac{u^{-n-1}}{n} - \frac{(t + \tau)u^{-n-2}}{n + 1} \right) \, du
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
= (n + 1)(\tau + t)^{n+1} \left( - \frac{u^{-n}}{n} + \frac{(t + \tau)u^{-n-1}}{n + 1} \right) \bigg|_{t+\tau}^\infty
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
= (n + 1)(\tau + t)^{n+1} \left[ \left( - \frac{(t + \tau)^{-n}}{n} + \frac{(t + \tau) (t + \tau)^{-n-1}}{n + 1} \right) - (0 - 0) \right]
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
= \frac{(t + \tau)(n + 1)}{n} - \frac{(t + \tau)(n + 1)}{n + 1} &amp;= (t + \tau)(n + 1) \left( \frac{1}{n} - \frac{1}{n + 1} \right)
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
= (t + \tau)(n + 1) \left( \frac{n + 1 - n}{n(n + 1)} \right) &amp;= \frac{t + \tau}{n}
\end{align*}\]</div>
</div>
</section>
<section id="normal-likelihood-with-improper-priors">
<h2>18. Normal Likelihood with Improper Priors<a class="headerlink" href="#normal-likelihood-with-improper-priors" title="Permalink to this headline">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be iid normals <span class="math notranslate nohighlight">\(N (\theta, \sigma^2)\)</span>, where</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the parameter of interest, and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is known. Assume a flat prior on <span class="math notranslate nohighlight">\(\theta\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\pi(\theta) = 1, \, -\infty &lt; \theta &lt; \infty
\]</div>
<p>Show that the posterior is</p>
<div class="math notranslate nohighlight">
\[
[\theta \mid X_1, \ldots, X_n] \sim N \left( \bar{X}, \frac{\sigma^2}{n} \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the mean of the observations.</p>
<ol class="arabic simple" start="2">
<li><p><span class="math notranslate nohighlight">\(\sigma^2\)</span> is the parameter of interest, and <span class="math notranslate nohighlight">\(\theta\)</span> is known. Let the prior on <span class="math notranslate nohighlight">\(\sigma^2\)</span> be</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\pi(\sigma^2) = \frac{1}{\sigma^2}, \quad \sigma^2 &gt; 0.
\]</div>
<p>Show that the posterior is inverse gamma</p>
<div class="math notranslate nohighlight">
\[
\sigma^2 \mid X_1, \ldots, X_n \sim \text{IG} \left( \frac{n}{2}, \frac{\sum_{i=1}^n (X_i - \theta)^2}{2} \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{IG}(a, b)\)</span> stands for the distribution with a density</p>
<div class="math notranslate nohighlight">
\[
f(y) = \frac{b^a}{\Gamma(a)} y^{-a-1} e^{-b/y}, \quad a, b &gt; 0, \, y \geq 0.
\]</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(x \mid \theta) &amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_i - \theta)^2}{2\sigma^2}} = C_0 e^{-\frac{1}{2\sigma^2} \sum_{i=1}(x_i - \theta)^2} \\
\pi(\theta \mid x) &amp;\propto f(x \mid \theta) \pi(\theta) \\
&amp;= C_0 e^{-\frac{1}{2\sigma^2} \sum_{i=1}(x_i - \theta)^2} \cdot 1 \\
&amp;\propto e^{-\frac{1}{2\sigma^2} \sum_{i=1} (x_i^2 - 2x_i\theta + \theta^2)} \\
&amp;\propto e^{-\frac{1}{2\sigma^2} \left( \sum_{i=1} x_i^2 - 2\theta \sum_{i=1} x_i + n\theta^2 \right)} \\
&amp;\propto e^{-\frac{1}{2\sigma^2} \left( n\theta^2 - 2\theta \sum_{i=1} x_i \right)} \\
&amp;= e^{-\frac{n}{2\sigma^2} \left( \theta^2 - 2\theta \frac{1}{n} \sum_{i=1} x_i \right)} \\
&amp;\propto e^{-\frac{1}{2 \left( \frac{\sigma^2}{n} \right)} \left( \theta - \frac{\sum_{i=1} x_i}{n} \right)^2} \\
&amp;\sim N \left( \bar{X}, \frac{\sigma^2}{n} \right)
\end{align*}\]</div>
<p>A note about the second to last proportion, this is just completing the square and discarding the extra bit needed to complete the square because it is proportional!</p>
<ol class="arabic simple" start="2">
<li></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_i - \theta)^2}{2\sigma^2}} &amp;= C_0 \sigma^{-n} e^{-\frac{1}{2\sigma^2} \sum_{i=1}(x_i - \theta)^2} \\
\pi(\sigma^2 \mid x) &amp;\propto f(x \mid \sigma^2) \pi(\sigma^2) \\
&amp;= C_0 \sigma^{-n} e^{-\frac{1}{2\sigma^2} \sum_{i=1}(x_i - \theta)^2} \cdot \frac{1}{\sigma^2} \\
&amp;\propto \sigma^{-(n+2)} e^{-\frac{1}{2\sigma^2} \sum_{i=1}(x_i - \theta)^2} \\
&amp;\propto (\sigma^2)^{-\left(\frac{n+2}{2}\right)} e^{-\frac{\sum_{i=1}(x_i - \theta)^2}{2\sigma^2}}
\end{align*}\]</div>
<p>This is proportional to an inverse gamma distribution but let us be explicit about the parameters. The <span class="math notranslate nohighlight">\((\sigma^2)^{-\left(\frac{n+2}{2}\right)}\)</span> is like <span class="math notranslate nohighlight">\(y^{-(a+1)}\)</span> so:</p>
<div class="math notranslate nohighlight">
\[
-\left(\frac{n + 2}{2}\right) = -(a + 1) \rightarrow n + 2 = 2a + 2 \rightarrow a = \frac{n}{2}
\]</div>
<p>The <span class="math notranslate nohighlight">\(e^{-\frac{\sum_{i=1}(x_i - \theta)^2}{2\sigma^2}}\)</span> is like <span class="math notranslate nohighlight">\(e^{-\frac{b}{y}}\)</span> so:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{i=1}(x_i - \theta)^2}{2} = b
\]</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./unit4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="SupplementaryExercises43.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supplementary Exercises 4.3</p>
      </div>
    </a>
    <a class="right-next"
       href="../unit5/bayesian-computation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Computation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mosaic-virus">2. Mosaic Virus</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#figo">3. FIGO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histocompatibility">4. Histocompatibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-fire-in-potters-lab">5. Neurons Fire in Potter’s Lab</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elicit-inverse-gamma-prior">6. Elicit Inverse Gamma Prior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derive-jeffreys-priors-for-poisson-lambda-bernoulli-p-and-geometric-p">7. Derive Jeffreys’ Priors for Poisson <span class="math notranslate nohighlight">\(\lambda\)</span>, Bernoulli <span class="math notranslate nohighlight">\(p\)</span>, and Geometric <span class="math notranslate nohighlight">\(p\)</span>.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-scenarios-for-the-probability-of-success">8. Two Scenarios for the Probability of Success</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jeffreys-prior-for-normal-precision">9. Jeffreys’ Prior for Normal Precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derive-jeffreys-prior-for-maxwells-theta">10. Derive Jeffreys’ Prior for Maxwell’s <span class="math notranslate nohighlight">\(\theta\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-jeffreys-priors">11. “Quasi” Jeffreys’ Priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#haldane-prior-for-binomial-p">12. Haldane Prior for Binomial p</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eliciting-a-normal-prior">13. Eliciting a Normal Prior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jigsaw">14. Jigsaw</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jeremy-and-poisson">15. Jeremy and Poisson</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#npeb-for-p-in-the-geometric-distribution">16. NPEB for <em>p</em> in the Geometric Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lifetimes-and-predictive-distribution">17. Lifetimes and Predictive Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-likelihood-with-improper-priors">18. Normal Likelihood with Improper Priors</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Aaron Reding
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>