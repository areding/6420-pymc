{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a549b39-51d5-49ec-bdad-e7b8a4ff97de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. Conditional Predictive Ordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf65ea-0582-4d38-9c11-19410d1ac307",
   "metadata": {},
   "source": [
    "From lecture, we  learned that the CPO can be defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{CPO}_{i} = f(y_{i}\\vert y_{-i})\n",
    "=\\int_{\\Theta}f(y_{i}\\vert \\theta)\\pi(\\theta \\vert y_{-i})d\\theta\n",
    "\\end{equation*}\n",
    "Where $y_{-i}$ denotes all of the data **exclusive of the $i$-th observation** .  Here, we will derive the formula for CPO (somewhat rigorously) and allude to how it is well-calculated from an appropriate number of posterior samples (less rigorously).\n",
    "\n",
    "\n",
    "Starting with Bayes' Theorem, and noting that the likelihood term $p(y\\vert \\theta)$ factorizes into $p(y_{i}\\vert \\theta)\\cdot p(y_{-i}\\vert \\theta)$ \n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta \\vert y) = \\frac{p(y\\vert \\theta) p(\\theta)}{p(y)}\n",
    "= \\frac{p(y_{i}\\vert \\theta)p(y_{-i}\\vert \\theta)p(\\theta)}{p(y)}\n",
    "\\end{equation*}\n",
    "Which gives us:\n",
    "\\begin{equation*}\n",
    "p(y_{-i}\\vert \\theta) = \\frac{p(y)p(\\theta \\vert y)}{p(y\\vert \\theta)p(\\theta)}\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{equation*}\n",
    "p(\\theta \\vert y_{-i}) = \\frac{p(y_{-i}\\vert \\theta)p(\\theta)}{p(y_{-i})}\n",
    "\\end{equation*}\n",
    "The ratio $\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)}$ is helpful here:\n",
    "\\begin{align*}\n",
    "\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)} &=\n",
    "\\frac{\\frac{p(y_{-i}\\vert \\theta)\\cancel{p(\\theta)}}{p(y_{-i})}}{\\frac{p(y\\vert \\theta)\\cancel{p(\\theta)}}{p(y)}}\n",
    "=\\frac{p(y_{-i}\\vert \\theta)p(y)}{p(y\\vert \\theta)p(y_{-i})}\\\\ &=\\frac{p(y)}{p(y_{-i})}\\cdot\\frac{\\cancel{p(y_{-i}\\vert \\theta)}}{p(y_{i}\\vert \\theta)\\cancel{p(y_{-i}\\vert \\theta)}}\\\\\n",
    "\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)} &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}\\\\\n",
    "\\frac{p(\\theta \\vert y_{-i})}{\\cancel{p(\\theta \\vert y)}} \\cancel{\\textcolor{blue}{p(\\theta\\vert y)}} &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}\\textcolor{blue}{p(\\theta\\vert y)}\\\\\n",
    "p(\\theta \\vert y_{-i}) &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)\\\\\n",
    "%first integral line\n",
    "\\int_{\\Theta}p(\\theta \\vert y_{-i})d\\theta &= \\frac{p(y)}{p(y_{-i})}\\cdot \\int_{\\Theta}\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)d\\theta\n",
    "\\end{align*}\n",
    "The left hand side above is a probability distribution integrated over the entire parameter space, so it equals 1.\n",
    "\n",
    "\\begin{align*}\n",
    "1 &= \\frac{p(y)}{p(y_{-i})}\\cdot \\int_{\\Theta}\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)d\\theta\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p(y_{-i})}{p(y)} = \\int_{\\theta}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg]\\cdot p(\\theta \\vert y)d\\theta\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "The right-hand side of equation (1) should remind you of LOTUS (Law of The Unconscious Statistician): It's just an expectation.  Recall that for a continuous random variable $X$ with density function $p(x)$, if $Y=g(x)$ then:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[Y]=\\mathbb{E}[g(x)]=\\int_{-\\infty}^{\\infty}g(x)\\cdot p(x)dx\n",
    "\\end{equation*}\n",
    "\n",
    "Since the density function is a posterior, will slightly modify the notation for the expectation operator:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{p(y_{-i})}{p(y)} = \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg]\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, the right-hand side of (1) represents the expectation over $\\theta$ where $\\theta$ is distributed according to $p(\\theta \\vert y)$.  Taking the reciprocal of both sides gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{p(y)}{p(y_{-i})} = \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\n",
    "\\end{equation*}\n",
    "\n",
    "Let's reframe $p(y)$.  It's the marginal distribution of all the data, but we could just as well think of it as the joint distribution of $y_{i}$ and $y_{-i}$: they're one in the same.  This will allow us to apply the definition of conditional probability\n",
    "\\begin{equation*}\n",
    "p(y) = p(y_{i} , y_{-i}) = \\textcolor{blue}{p(y_{i}\\vert y_{-i})}p(y_{-i})\n",
    "\\end{equation*}\n",
    "notice that the blue factor is the $\\text{CPO}_{i}$ defined at the beginning.  Since (6) can be re-written as:\n",
    "\\begin{align*}\n",
    "\\frac{p(y_{i}\\vert y_{-i})\\cancel{p(y_{-i}})}{\\cancel{p(y_{-i})}} &= \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\\\\\n",
    "p(y_{i}\\vert y_{-i})&= \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\\\\\n",
    "&=\\text{CPO}_{i}\\blacksquare\n",
    "\\end{align*}\n",
    "As long as the MCMC algorithm is correctly sampling the posterior, this is well approximated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\text{CPO}}_{i}=\n",
    "\\bigg(\n",
    "\\frac{1}{B}\\sum_{b=1}^{B}\\frac{1}{f(y_{i}\\vert \\theta^{(b)})}\n",
    "\\bigg)^{-1}\n",
    "\\end{equation*}\n",
    "Where $\\theta^{(b)}$ is an individual posterior sample.  \n",
    "\n",
    "1. For each posterior sample $\\theta^{(b)}$:\n",
    "\n",
    "   a. Evaluate the likelihood at the same $y_{i}$ but use the posterior sample $\\theta^{(b)}$ as the parameter value.\n",
    "\n",
    "   b. Take the reciprocal of the calculation above.\n",
    "\n",
    "2. Take the mean of all values from step (1)\n",
    "\n",
    "3. Take the reciprocal of the value from step (2), this is the final calculation for $\\text{CPO}_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce548697",
   "metadata": {},
   "source": [
    "## GESELL DEVELOPMENTAL SCHEDULES\n",
    "\n",
    "\n",
    "Gesell Developmental Schedules measure child development. The scoring is devised by the American child psychologist and pediatrician Arnold Gesell who founded the Clinic of Child Development at Yale in 1911 and directed it for many years. The Gesell Developmental Schedules are a gauge of the status of a child's motor and language development and personal-social and adaptive behaviors.\n",
    "\n",
    "For 21 children the age in months at which they first spoke and their Gesell Adaptive Score, which is the result of an aptitude test taken much later was recorded. These data were originally collected by L. M. Linde of UCLA but were first published by M. R. Mickey, O. J. Dunn, and V. Clark, \"Note on the use of stepwise regression in detecting outliers,\" *Computers and Biomedical Research*, 1 (1967), pp.105-111.\n",
    "\n",
    "The data have been used by several authors, e.g., N. R. Draper and J. A. John, \"Influential observations and outliers in regression,\"\n",
    "*Technometrics*, 23 (1981), pp. 21-26."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad98562",
   "metadata": {},
   "source": [
    "Let $X$ be the age in months a child speaks his/her first word and let $Y$ be the Gesell adaptive score, a measure of a child's aptitude (observed later on):\n",
    "\n",
    "| Child  | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8  | 9  | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 |\n",
    "|--------|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\n",
    "| Age    | 15 | 26 | 10 | 9  | 15 | 20 | 18 | 11 | 8  | 20 | 7  | 9  | 10 | 11 | 11 | 10 | 12 | 42 | 17 | 11 | 10 |\n",
    "| Score  | 95 | 71 | 83 | 91 | 102| 87 | 93 |100 |104 | 94 |113 | 96 | 83 | 84 |102 |100 |105 | 57 |121 | 86 |100 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e97da4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
