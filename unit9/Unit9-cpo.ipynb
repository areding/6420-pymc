{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a549b39-51d5-49ec-bdad-e7b8a4ff97de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. Conditional Predictive Ordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf65ea-0582-4d38-9c11-19410d1ac307",
   "metadata": {},
   "source": [
    "From lecture, we  learned that the CPO can be defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{CPO}_{i} = f(y_{i}\\vert y_{-i})\n",
    "=\\int_{\\Theta}f(y_{i}\\vert \\theta)\\pi(\\theta \\vert y_{-i})d\\theta\n",
    "\\end{equation*}\n",
    "Where $y_{-i}$ denotes all of the data **exclusive of the $i$-th observation** .  In the lecture we learn that we can estimate this by using MCMC samples:\n",
    "\\begin{equation*}\n",
    "\\hat{\\text{CPO}}_{i}=\n",
    "\\bigg(\n",
    "\\frac{1}{B}\\sum_{b=1}^{B}\\frac{1}{f(y_{i}\\vert \\theta^{(b)})}\n",
    "\\bigg)^{-1}\n",
    "\\end{equation*}\n",
    "We denote posterior samples as $\\theta^{(b)}$, drawn from $\\pi(\\theta \\vert y)$, and evaluate the likelihood at each $y_{i}$.  At a high level, here is what we need to do:\n",
    "\n",
    "For each posterior sample $\\theta^{(b)}$:\n",
    "\n",
    " - Evaluate the likelihood at the same $y_{i}$ but use the posterior sample $\\theta^{(b)}$ as the parameter value.\n",
    "\n",
    " - Take the reciprocal of the calculation above.\n",
    "\n",
    "Then take the mean of the reciprocals.  Taking the reciprocal of that mean is an MCMC estimate of $\\text{CPO}_{i}$.\n",
    "\n",
    "Since we are estimating CPO using a mean, it might be prudent to show that CPO is simply an expected value.  Lets build some intuition behind the formula above.  \n",
    "\n",
    "\n",
    "Starting with Bayes' Theorem, and noting that the likelihood term $p(y\\vert \\theta)$ factorizes into $p(y_{i}\\vert \\theta)\\cdot p(y_{-i}\\vert \\theta)$ \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta \\vert y) = \\frac{p(y\\vert \\theta) p(\\theta)}{p(y)}\n",
    "= \\frac{p(y_{i}\\vert \\theta)p(y_{-i}\\vert \\theta)p(\\theta)}{p(y)}\n",
    "\\end{equation}\n",
    "Which gives us:\n",
    "\\begin{equation}\n",
    "p(y_{-i}\\vert \\theta) = \\frac{p(y)p(\\theta \\vert y)}{p(y\\vert \\theta)p(\\theta)}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "p(\\theta \\vert y_{-i}) = \\frac{p(y_{-i}\\vert \\theta)p(\\theta)}{p(y_{-i})}\n",
    "\\end{equation}\n",
    "The ratio $\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)}$ is helpful here:\n",
    "\\begin{align*}\n",
    "\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)} &=\n",
    "\\frac{\\frac{p(y_{-i}\\vert \\theta)\\cancel{p(\\theta)}}{p(y_{-i})}}{\\frac{p(y\\vert \\theta)\\cancel{p(\\theta)}}{p(y)}}\n",
    "=\\frac{p(y_{-i}\\vert \\theta)p(y)}{p(y\\vert \\theta)p(y_{-i})}\\\\ &=\\frac{p(y)}{p(y_{-i})}\\cdot\\frac{\\cancel{p(y_{-i}\\vert \\theta)}}{p(y_{i}\\vert \\theta)\\cancel{p(y_{-i}\\vert \\theta)}}\\\\\n",
    "\\frac{p(\\theta \\vert y_{-i})}{p(\\theta \\vert y)} &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}\\\\\n",
    "\\frac{p(\\theta \\vert y_{-i})}{\\cancel{p(\\theta \\vert y)}} \\cancel{\\textcolor{blue}{p(\\theta\\vert y)}} &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}\\textcolor{blue}{p(\\theta\\vert y)}\\\\\n",
    "p(\\theta \\vert y_{-i}) &= \\frac{p(y)}{p(y_{-i})}\\cdot\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)\\\\\n",
    "%first integral line\n",
    "\\int_{\\Theta}p(\\theta \\vert y_{-i})d\\theta &= \\frac{p(y)}{p(y_{-i})}\\cdot \\int_{\\Theta}\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)d\\theta\n",
    "\\end{align*}\n",
    "The left hand side above is a probability distribution integrated over the entire parameter space, so it equals 1.\n",
    "\n",
    "\\begin{align*}\n",
    "1 &= \\frac{p(y)}{p(y_{-i})}\\cdot \\int_{\\Theta}\\frac{1}{p(y_{i}\\vert \\theta)}p(\\theta\\vert y)d\\theta\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p(y_{-i})}{p(y)} = \\int_{\\theta}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg]\\cdot p(\\theta \\vert y)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "The right-hand side above should remind you of LOTUS (Law of The Unconscious Statistician): It's just an expectation.  Recall that for a continuous random variable $X$ with density function $p(x)$, if $Y=g(x)$ then:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[Y]=\\mathbb{E}[g(x)]=\\int_{-\\infty}^{\\infty}g(x)\\cdot p(x)dx\n",
    "\\end{equation*}\n",
    "\n",
    "In the left-hand side of line 4, the density function is a posterior so we will modify the notation slightly:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p(y_{-i})}{p(y)} = \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the left-hand side represents the expectation over $\\theta$ where $\\theta$ is distributed according to $p(\\theta \\vert y)$.  Taking the reciprocal gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p(y)}{p(y_{-i})} = \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "Let's reframe $p(y)$.  It's the marginal distribution of all the data, but we could just as well think of it as the joint distribution of $y_{i}$ and $y_{-i}$: they're one in the same.  This will allow us to apply the definition of conditional probability\n",
    "\\begin{equation*}\n",
    "p(y) = p(y_{i} , y_{-i}) = p(y_{i}\\vert y_{-i})p(y_{-i})\n",
    "\\end{equation*}\n",
    "So (6) can be re-written as:\n",
    "\\begin{align*}\n",
    "\\frac{p(y_{i}\\vert y_{-i})\\cancel{p(y_{-i}})}{\\cancel{p(y_{-i})}} &= \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\\\\\n",
    "p(y_{i}\\vert y_{-i})&= \n",
    "\\bigg( \\mathbb{E}_{\\theta \\sim p(\\theta \\vert y)}\\bigg[ \\frac{1}{p(y_{i}\\vert \\theta)} \\bigg] \\bigg)^{-1}\n",
    "\\end{align*}\n",
    "Note that the left-hand side is how we defined CPO above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce548697",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
