{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85facc28",
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "\n",
    "%load_ext watermark\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff26b14",
   "metadata": {},
   "source": [
    "# Dugongs\n",
    "This example is the first example of dealing with missing data.\n",
    "\n",
    "It is adapted from [unit 6: dugongsmissing.odc](https://raw.githubusercontent.com/areding/6420-pymc/main/original_examples/Codes4Unit6/dugongsmissing.odc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ed611",
   "metadata": {},
   "source": [
    "## Associated lecture video: Unit 6 Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca3f11",
   "metadata": {},
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed?v=xomK4tcePmc&list=PLv0FeK5oXK4l-RdT6DWJj0_upJOG2WKNO&index=57\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56e8ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem statement\n",
    "Carlin and Gelfand (1991) investigated the age (x) and length (y) of 27 captured dugongs (sea cows). Estimate parameters in a nonlinear growth model.\n",
    "\n",
    "### References\n",
    "\n",
    "Data provided by Ratkowsky (1983).\n",
    "\n",
    "Carlin, B. and Gelfand, B. (1991). An Iterative Monte Carlo Method for \n",
    "Nonconjugate Bayesian Analysis, Statistics and Computing, 1, (2), 119\u0013â€ 128.\n",
    "\n",
    "Ratkowsky, D. (1983). Nonlinear regression modeling: A unified practical approach. M. Dekker, NY, viii, 276 p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55869d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "X = [1.0, 1.5, 1.5, 1.5, 2.5, 4.0, 5.0, 5.0, 7.0, 8.0, 8.5, 9.0, 9.5, \n",
    "     9.5, 10.0, 12.0, 12.0, 13.0, 13.0, 14.5, 15.5, 15.5, 16.5, 17.0,\n",
    "     22.5, 29.0, 31.5]\n",
    "y = [1.80, 1.85, 1.87, -1, 2.02, 2.27, 2.15, 2.26, 2.47, 2.19, 2.26,\n",
    "     2.40, 2.39, 2.41, 2.50, 2.32, 2.32, 2.43, 2.47, 2.56, 2.65, 2.47,\n",
    "     2.64, 2.56, 2.70, 2.72, -1]\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e3420",
   "metadata": {},
   "source": [
    "PyMC imputes missing data automatically, similar to BUGS, but it requires the missing data be input as a NumPy masked array. See the NumPy docs for [np.ma.masked_values()](https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_values.html). For ```y```, above, you can enter whatever number at the missing data positions, then plug it into the ```value``` parameter below. Just make sure the number you're using isn't actually in the data. I chose -1 above.\n",
    "\n",
    "Note that this method only works for missing *observed* data. We'll learn about missing data in X later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daeb3b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data=[1.8, 1.85, 1.87, --, 2.02, 2.27, 2.15, 2.26, 2.47,\n",
       "                   2.19, 2.26, 2.4, 2.39, 2.41, 2.5, 2.32, 2.32, 2.43,\n",
       "                   2.47, 2.56, 2.65, 2.47, 2.64, 2.56, 2.7, 2.72, --],\n",
       "             mask=[False, False, False,  True, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False,  True],\n",
       "       fill_value=-1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ma.masked_values(y, value=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14956c2",
   "metadata": {},
   "source": [
    "Check it out! The array now has a mask attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8b2b77",
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/pymc/model.py:1325: ImputationWarning: Data in likelihood contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/pymc/aesaraf.py:996: UserWarning: The parameter 'updates' of aesara.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (collections.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n",
      "  aesara_function = aesara.function(\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta, gamma, sigma, likelihood_missing]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='42000' class='' max='42000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [42000/42000 00:36<00:00 Sampling 4 chains, 603 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/aaron/mambaforge/envs/pymc_env/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "Sampling 4 chains for 500 tune and 10_000 draw iterations (2_000 + 40_000 draws total) took 44 seconds.\n",
      "There were 24 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9083, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 578 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.7197, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9516, but should be close to 0.8. Try to increase the number of tuning steps.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as m:\n",
    "    # priors\n",
    "    alpha = pm.Uniform(\"alpha\", 0, 100)\n",
    "    beta = pm.Uniform(\"beta\", 0, 100)\n",
    "    gamma = pm.Uniform(\"gamma\", 0, 1)\n",
    "    sigma = pm.math.exp(pm.Uniform(\"sigma\", -10, 10))\n",
    "\n",
    "    mu = alpha - beta * gamma**X\n",
    "\n",
    "    likelihood = pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y)\n",
    "\n",
    "    # start sampling\n",
    "    trace = pm.sample(\n",
    "        10000,\n",
    "        chains=4,\n",
    "        tune=500,\n",
    "        cores=4,\n",
    "        init=\"jitter+adapt_diag\",\n",
    "        random_seed=1,\n",
    "        return_inferencedata=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a47b9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>likelihood_missing[0]</th>\n",
       "      <td>1.904</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.116</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16516.0</td>\n",
       "      <td>21145.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likelihood_missing[1]</th>\n",
       "      <td>2.692</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.435</td>\n",
       "      <td>2.932</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7274.0</td>\n",
       "      <td>6980.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>2.738</td>\n",
       "      <td>0.139</td>\n",
       "      <td>2.517</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4447.0</td>\n",
       "      <td>2155.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.785</td>\n",
       "      <td>1.204</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5218.0</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.888</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>2796.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>-2.341</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-2.638</td>\n",
       "      <td>-2.039</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5704.0</td>\n",
       "      <td>3755.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean     sd  hdi_2.5%  hdi_97.5%  mcse_mean  mcse_sd  \\\n",
       "likelihood_missing[0]  1.904  0.110     1.684      2.116      0.001    0.001   \n",
       "likelihood_missing[1]  2.692  0.127     2.435      2.932      0.002    0.001   \n",
       "alpha                  2.738  0.139     2.517      3.000      0.003    0.002   \n",
       "beta                   0.990  0.116     0.785      1.204      0.002    0.001   \n",
       "gamma                  0.888  0.035     0.818      0.952      0.000    0.000   \n",
       "sigma                 -2.341  0.154    -2.638     -2.039      0.002    0.001   \n",
       "\n",
       "                       ess_bulk  ess_tail  r_hat  \n",
       "likelihood_missing[0]   16516.0   21145.0    1.0  \n",
       "likelihood_missing[1]    7274.0    6980.0    1.0  \n",
       "alpha                    4447.0    2155.0    1.0  \n",
       "beta                     5218.0    2215.0    1.0  \n",
       "gamma                    4975.0    2796.0    1.0  \n",
       "sigma                    5704.0    3755.0    1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2254406",
   "metadata": {},
   "source": [
    "This output is very close to the BUGS results if you use the inits labeled Alternative (full) inits (initializing the missing values at 1):\n",
    "\n",
    "|           | mean    | sd      | MC_error | val2.5pc | median  | val97.5pc | start | sample |\n",
    "|-----------|---------|---------|----------|----------|---------|-----------|-------|--------|\n",
    "| alpha     | 2.731   | 0.1206  | 0.003167 | 2.551    | 2.711   | 3.025     | 1001  | 100000 |\n",
    "| beta      | 0.9846  | 0.1021  | 0.002008 | 0.8053   | 0.9773  | 1.212     | 1001  | 100000 |\n",
    "| gamma     | 0.8874  | 0.03381 | 7.64E-04 | 0.8124   | 0.8908  | 0.9427    | 1001  | 100000 |\n",
    "| log.sigma | -2.342  | 0.1543  | 7.83E-04 | -2.622   | -2.349  | -2.018    | 1001  | 100000 |\n",
    "| sigma     | 0.09733 | 0.0155  | 7.96E-05 | 0.07267  | 0.09547 | 0.1329    | 1001  | 100000 |\n",
    "| y[4]      | 1.906   | 0.1098  | 6.23E-04 | 1.689    | 1.906   | 2.123     | 1001  | 100000 |\n",
    "| y[27]     | 2.69    | 0.1255  | 0.001916 | 2.446    | 2.689   | 2.941     | 1001  | 100000 |\n",
    "\n",
    "\n",
    "If you use the first set of inits, as Professor Vidakovic did in the video, BUGS gives the y[4] mean as 2.047 and y[27] as 3.04. All the other variables are very different here, too. This is quite a mystery - as far as I can tell, the only difference is that the missing values use BUGS-generated inits for the following results. If someone figures out what's going on here, let me know!\n",
    "\n",
    "|       | mean   | sd       | MC_error | val2.5pc | median | val97.5pc | start | sample |\n",
    "|-------|--------|----------|----------|----------|--------|-----------|-------|--------|\n",
    "| alpha | 32.54  | 3.698    | 0.2076   | 23.51    | 32.69  | 40.31     | 1001  | 100000 |\n",
    "| beta  | 30.55  | 3.697    | 0.2076   | 21.52    | 30.69  | 38.31     | 1001  | 100000 |\n",
    "| gamma | 0.9989 | 2.08E-04 | 8.70E-06 | 0.9984   | 0.9989 | 0.9992    | 1001  | 100000 |\n",
    "| sigma | 0.1328 | 0.02071  | 8.17E-05 | 0.09964  | 0.1303 | 0.1805    | 1001  | 100000 |\n",
    "| y[4]  | 2.047  | 0.1424   | 5.18E-04 | 1.766    | 2.046  | 2.329     | 1001  | 100000 |\n",
    "| y[27] | 3.04   | 0.1606   | 7.42E-04 | 2.722    | 3.04   | 3.358     | 1001  | 100000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9293d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.4\n",
      "IPython version      : 8.3.0\n",
      "\n",
      "numpy     : 1.22.3\n",
      "pymc      : 4.0.0b5\n",
      "matplotlib: 3.5.2\n",
      "arviz     : 0.12.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96057a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
