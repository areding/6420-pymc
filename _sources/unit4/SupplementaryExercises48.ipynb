{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ada150",
   "metadata": {},
   "source": [
    "# Supplementary Exercises 4.8\n",
    "\n",
    "```{warning}\n",
    "This page contains solutions! We recommend attempting each problem before peeking.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8df828",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The old question 1 was a duplicate of [4.3 question 16](https://areding.github.io/6420-pymc/unit4/SupplementaryExercises43.html#counts-of-alpha), so we're going to start numbering with 2 in order to maintain consistency with the old materials.\n",
    "```\n",
    "\n",
    "## 2. Mosaic Virus\n",
    "\n",
    "A single leaf is taken from each of 8 different tobacco plants. Each leaf is then divided in half, and given one of two preparations of mosaic virus. Researchers wanted to examine if there is a difference in the mean number of lesions from the two preparations. Here is the raw data:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\text{Plant} & \\text{Prep 1} & \\text{Prep 2} \\\\\n",
    "1 & 38 & 29 \\\\\n",
    "2 & 40 & 35 \\\\\n",
    "3 & 26 & 31 \\\\\n",
    "4 & 33 & 31 \\\\\n",
    "5 & 21 & 14 \\\\\n",
    "6 & 27 & 37 \\\\\n",
    "7 & 41 & 22 \\\\\n",
    "8 & 36 & 25 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Assume the normal distribution for the difference between the populations/samples. Using a PPL find:\n",
    "\n",
    "1. the 95% credible set for $\\mu_1 - \\mu_2$, and\n",
    "2. posterior probability of hypothesis $H_1: \\mu_1 - \\mu_2 \\geq 0$.\n",
    "\n",
    "Use noninformative priors.\n",
    "\n",
    "Hint: Since this is a paired two sample problem, a single model should be placed on the difference.\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d600df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "prep1 = np.array([38, 40, 26, 33, 21, 27, 41, 36])\n",
    "prep2 = np.array([29, 35, 31, 31, 14, 37, 22, 25])\n",
    "diff = prep1 - prep2\n",
    "\n",
    "with pm.Model() as m:\n",
    "    tau = pm.Gamma(\"precision\", 0.001, 0.001)\n",
    "    mu = pm.Normal(\"mean\", 0, tau=0.0001)\n",
    "    sigma2 = pm.Deterministic(\"variance\", 1 / tau)\n",
    "\n",
    "    pm.Normal(\"likelihood\", mu, tau=tau, observed=diff)\n",
    "\n",
    "    trace = pm.sample(3000)\n",
    "\n",
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a4e7c",
   "metadata": {},
   "source": [
    "## 3. FIGO\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c6065",
   "metadata": {},
   "source": [
    "## 4. Histocompatibility\n",
    "\n",
    "A patient who is waiting for an organ transplant needs a histocompatible donor who matches the patient’s human leukocyte antigen (HLA) type.\n",
    "\n",
    "For a given patient, the number of matching donors per 1000 National Blood Bank records is modeled as Poisson with unknown rate $\\lambda$. If a randomly selected group of 1000 records showed exactly one match, estimate $\\lambda$ in Bayesian fashion.\n",
    "\n",
    "For $\\lambda$​ assume:\n",
    "\n",
    "1. Gamma $\\text{Ga}(\\alpha=2, \\beta=1)$​ prior;\n",
    "2. flat prior $\\lambda = 1$, for $\\lambda > 0$​;\n",
    "3. invariance prior $\\pi(\\lambda) = \\frac{1}{\\lambda}$, for $\\lambda > 0$​;\n",
    "4. Jeffreys prior $\\pi(\\lambda) = \\sqrt{\\frac{1}{\\lambda}}$, for $\\lambda > 0$.\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "1. Gamma $\\text{Ga}(\\alpha=2, \\beta=1)$​ prior;\n",
    "\n",
    "Poisson PDF $\\propto e^{-\\lambda}\\lambda^k$​​\n",
    "\n",
    "To shake things up, let's generalize to multiple independent datapoints, even though we only have a single datapoint equalling 1 for this problem.\n",
    "\n",
    "$\\prod_{i=1}^{n} e^{-\\lambda}\\lambda^{k_i} = e^{- n\\lambda}\\lambda^{\\sum_{i=1}^{n} k_i}$​\n",
    "\n",
    "Gamma PDF $\\propto x^{\\alpha -1}e^{-\\beta x}$​ \n",
    "\n",
    "$$ \n",
    "\n",
    "\\begin{align*} \\pi(\\lambda \\mid k) &\\propto \\left(e^{- n\\lambda}\\lambda^{\\sum_{i=1}^{n} k_i}\\right) \\left(\\lambda^{\\alpha -1}e^{-\\beta \\lambda}\\right) \\\\\n",
    "\n",
    "&\\propto \\lambda^{(\\alpha -1) + \\sum_{i=1}^{n} k_i}e^{-\\beta \\lambda - n\\lambda} \\\\ \n",
    "\n",
    "&\\propto \\lambda^{(\\alpha + \\sum_{i=1}^{n} k_i) - 1}e^{-(\\beta + n)\\lambda} \\\\\n",
    "\n",
    "&= Ga(\\alpha + \\sum_{i=1}^{n} k_i, \\beta + n) \n",
    "\n",
    "\\end{align*} \n",
    "\n",
    "$$ \n",
    "\n",
    "We recognize the $Ga(\\alpha + \\sum_{i=1}^{n} k_i, \\beta + n)$ posterior, which comes out to $Ga(3, 2)$ in this case. Our Bayes estimate is then the mean of the posterior, which is $\\frac{\\alpha}{\\beta} = \\frac{3}{2}$.\n",
    "\n",
    "2. flat prior $\\lambda = 1$, for $\\lambda > 0$​;\n",
    "\n",
    "Then go on with the same procedure: \n",
    "\n",
    "$$ \n",
    "\n",
    "\\begin{align*} \n",
    "\n",
    "\\pi(\\lambda \\mid k) &\\propto \\left(e^{- n\\lambda}\\lambda^{\\sum_{i=1}^{n} k_i}\\right) \\mathbf{1}(\\lambda > 0) \\\\\n",
    "\n",
    "&\\propto \\lambda^{\\left(\\sum_{i=1}^{n} k_i\\right) + 1 - 1} e^{- n\\lambda} \\mathbf{1}(\\lambda > 0) \\\\\n",
    "\n",
    "&= Ga(1 + \\sum_{i=1}^{n} k_i, n) \n",
    "\n",
    "\\end{align*} \n",
    "\n",
    "$$ \n",
    "\n",
    "Which in our case would be $Ga(2, 1)$ with a mean of 2.\n",
    "\n",
    "3. invariance prior $\\pi(\\lambda) = \\frac{1}{\\lambda}$, for $\\lambda > 0$​; \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\pi(\\lambda \\mid k) & \\propto \\left(e^{- n\\lambda}\\lambda^{\\sum_{i=1}^{n} k_i}\\right) \\frac{1}{\\lambda}\\mathbf{1}(\\lambda > 0) \\\\\n",
    "&\\propto e^{-n\\lambda} \\lambda^{-1 + \\sum_{i=1}^{n} k_i} \\mathbf{1}(\\lambda > 0) \\\\\n",
    "& = Ga(\\sum_{i=1}^{n} k_i, n) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "We identify the $Ga(1, 1)$ distribution, which has a mean of 1. Equivalently, the $Exp(1)$ distribution.\n",
    "\n",
    "4. Jeffreys prior $\\pi(\\lambda) = \\sqrt{\\frac{1}{\\lambda}}$, for $\\lambda > 0$.\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{align*} \n",
    "\n",
    "\\pi(\\lambda \\mid k) &\\propto \\left(e^{- n\\lambda}\\lambda^{\\sum_{i=1}^{n} k_i}\\right) \\times \\sqrt{\\frac{1}{\\lambda}}\\mathbf{1}(\\lambda > 0) \\\\\n",
    "\n",
    "&\\propto \\lambda^{- 1/2 + \\sum_{i=1}^{n} k_i} e^{-n\\lambda} \\mathbf{1}(\\lambda > 0) \\\\\n",
    "\n",
    "& \\propto \\lambda^{- 1 + \\left(1/2 + \\sum_{i=1}^{n} k_i\\right)} e^{-n\\lambda} \\mathbf{1}(\\lambda > 0) \\\\\n",
    "\n",
    "&= Ga(1/2 + \\sum_{i=1}^{n} k_i, n) \n",
    "\n",
    "\\end{align*} \n",
    "\n",
    "$$ \n",
    "\n",
    "In our case the posterior is $Ga(3/2, 1)$ with a mean of $3/2$.\n",
    "\n",
    "Note that the priors in (b-d) are not proper densities (the integrals are not finite), however, the resulting posteriors are proper.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08abe18",
   "metadata": {},
   "source": [
    "## 5. Neurons Fire in Potter's Lab\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae97d1",
   "metadata": {},
   "source": [
    "## 6. Elicit Inverse Gamma Prior\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8714a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Derive Jeffreys’ Priors for Poisson $\\lambda$, **Bernoulli** $p$, **and Geometric** $p$.\n",
    "\n",
    "Recall that Jeffreys’ prior for parameter $\\theta$ in the likelihood $f(x | \\theta)$ is defined as\n",
    "\n",
    "$$\\pi(\\theta) \\propto \\left| \\text{det}(I(\\theta)) \\right|^{1/2}$$\n",
    "\n",
    "where, for univariate parameters,\n",
    "\n",
    "$$I(\\theta) = E \\left[ \\left( \\frac{d \\log f(x | \\theta)}{d\\theta} \\right)^2 \\right] = -E \\left[ \\frac{d^2 \\log f(x | \\theta)}{d\\theta^2} \\right]$$\n",
    "\n",
    "and expectation is taken with respect to the random variable $X \\sim f(x | \\theta)$.\n",
    "\n",
    "**(a) Show that Jeffreys’ prior for Poisson distribution** $f(x | \\lambda) = \\frac{\\lambda^x}{x!} e^{-\\lambda}$, $\\lambda \\geq 0$, **is** $\\pi(\\lambda) = \\sqrt{\\frac{1}{\\lambda}}$.\n",
    "\n",
    "**(b) Show that Jeffreys’ prior for Bernoulli distribution** $f(x | p) = p^x (1 - p)^{1-x}$, $0 \\leq p \\leq 1$, **is** $\\pi(p) \\propto \\frac{1}{\\sqrt{p(1-p)}}$, which is the beta $\\text{Be}(1/2, 1/2)$ distribution (or Arcsin distribution).\n",
    "\n",
    "**(c) Show that Jeffreys’ prior for Geometric distribution** $f(x | p) = (1 - p)^{x-1} p$, $x = 1, 2, \\ldots$ ; $0 \\leq p \\leq 1$, **is** $\\pi(p) \\propto \\frac{1}{p \\sqrt{1-p}}$.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Derive Jeffreys’ Priors for Poisson** $\\lambda$, **Bernoulli** $p$, **and Geometric** $p$.\n",
    "\n",
    "**(a)**\n",
    "For the Poisson distribution with likelihood function:\n",
    "$$f(x | \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}$$\n",
    "\n",
    "First, we differentiate the log-likelihood with respect to $\\lambda$:\n",
    "\n",
    "$$\\frac{d}{d\\lambda} \\log \\left( \\frac{\\lambda^x e^{-\\lambda}}{x!} \\right) = \\frac{d}{d\\lambda} (x \\log \\lambda - \\lambda)$$\n",
    "\n",
    "Which gives:\n",
    "\n",
    "$$\\frac{x}{\\lambda} - 1$$\n",
    "\n",
    "Now, the Fisher Information $I(\\lambda)$ is given by:\n",
    "\n",
    "$$I(\\lambda) = E\\left[ \\left( \\frac{x}{\\lambda} - 1 \\right)^2 \\right] = \\frac{E[x^2]}{\\lambda^2} - \\frac{2E[x]}{\\lambda} + 1$$\n",
    "\n",
    "Given that $E[x^2] = Var(x) + (E[x])^2$ and for a Poisson distribution, $E[x] = \\lambda$ and $Var(x) = \\lambda$:\n",
    "\n",
    "$$E[x^2] = \\lambda + \\lambda^2$$\n",
    "\n",
    "Substituting this in, we get:\n",
    "\n",
    "$$I(\\lambda) = \\frac{1}{\\lambda}$$\n",
    "\n",
    "Thus, the Jeffreys’ prior is:\n",
    "\n",
    "$$\\pi(\\lambda) \\propto \\sqrt{\\frac{1}{\\lambda}}$$\n",
    "\n",
    "**(b)**\n",
    "For the Bernoulli distribution:\n",
    "$$f(x | p) = p^x (1-p)^{1-x}$$\n",
    "\n",
    "The log-likelihood is:\n",
    "\n",
    "$$L = x \\log(p) + (1 - x) \\log(1 - p)$$\n",
    "\n",
    "Differentiating $L$ with respect to $p$ we get:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p} = \\frac{x}{p} - \\frac{1-x}{1-p}$$\n",
    "\n",
    "And the second derivative is:\n",
    "\n",
    "$$\\frac{\\partial^2 L}{\\partial p^2} = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}$$\n",
    "\n",
    "For a Bernoulli distribution, $E[x] = p$. The Fisher Information $I(p)$ is:\n",
    "\n",
    "$$I(p) = \\frac{1}{p(1-p)}$$\n",
    "\n",
    "So, the Jeffreys’ prior is:\n",
    "\n",
    "$$\\pi(p) \\propto \\frac{1}{\\sqrt{p(1-p)}}$$\n",
    "\n",
    "**(c)**\n",
    "For the Geometric distribution:\n",
    "\n",
    "$$f(x | p) = (1-p)^{x-1} p$$\n",
    "\n",
    "The log-likelihood is:\n",
    "\n",
    "$$L = (x-1) \\log(1-p) + \\log(p)$$\n",
    "\n",
    "Differentiating $L$ with respect to $p$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p} = \\frac{1}{p} - \\frac{x-1}{1-p}$$\n",
    "\n",
    "And the second derivative is:\n",
    "\n",
    "$$\\frac{\\partial^2 L}{\\partial p^2} = -\\frac{1}{p^2} - \\frac{x-1}{(1-p)^2}$$\n",
    "\n",
    "For a Geometric distribution, $E[x] = \\frac{1}{p}$. The Fisher Information $I(p)$ is:\n",
    "\n",
    "$$I(p) = \\frac{1}{p^2(1-p)}$$\n",
    "\n",
    "So, the Jeffreys’ prior is:\n",
    "\n",
    "$$\\pi(p) \\propto \\frac{1}{p \\sqrt{1-p}}$$\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862aa57",
   "metadata": {},
   "source": [
    "## 8. Two Scenarios for the Probability of Success\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213f30d",
   "metadata": {},
   "source": [
    "Is there a typo on Q 8- b? \n",
    "\n",
    "It says Be(1, 21/2). But Posterior Expectation is 2/21. \n",
    "\n",
    "I got Be(1, 19/2) which results in 2/21 with the formula alpha. / (alpha + beta)\n",
    "\n",
    "Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9210154",
   "metadata": {},
   "source": [
    "## 9. Jeffreys' Prior for Normal Precision\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667ffcb",
   "metadata": {},
   "source": [
    "## 10. Derive Jeffreys' Prior for Maxwell's $\\theta$\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac512ed",
   "metadata": {},
   "source": [
    "## 11. \"Quasi\" Jeffreys' Priors\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b176011",
   "metadata": {},
   "source": [
    "## 12. Haldane Prior for Binomial p\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb35e99",
   "metadata": {},
   "source": [
    "## 13. Eliciting a Normal Prior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e59f6",
   "metadata": {},
   "source": [
    "## 14. Jigsaw\n",
    "\n",
    "An experiment with a sample of 18 nursery-school children involved the elapsed time required to put together a small jigsaw puzzle. The times were: \n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a4181",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/19OEW7E1wOfcvdpytEiLQ--VF4YlsRTcD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f615f2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec88db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    3.1,\n",
    "    3.2,\n",
    "    3.4,\n",
    "    3.6,\n",
    "    3.7,\n",
    "    4.2,\n",
    "    4.3,\n",
    "    4.5,\n",
    "    4.7,\n",
    "    5.2,\n",
    "    5.6,\n",
    "    6.0,\n",
    "    6.1,\n",
    "    6.6,\n",
    "    7.3,\n",
    "    8.2,\n",
    "    10.8,\n",
    "    13.6,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161ce82",
   "metadata": {},
   "source": [
    "Assume that data are coming from a normal distribution $N (\\mu, \\sigma^2)$ with $\\sigma^2 = 8$. For parameter $\\mu$, set a normal prior with mean 5 and variance 6.\n",
    "\n",
    "------------------\n",
    "\n",
    "(a) Find the Bayes estimator and 95% credible set for population mean $\\mu$.\n",
    "\n",
    "We can define our model like this:\n",
    "$$\n",
    "\\begin{align}\n",
    "x_i|\\mu &\\sim N(\\mu, 8) \\\\\n",
    "\\mu & \\sim N(5, 6)\n",
    "\\end{align}\n",
    "$$\n",
    "This is the Normal-Normal conjugate pair for a fixed variance and random mean, with $n=18$ and $\\bar{X} \\approx 5.78333$\n",
    "\n",
    "Our posterior is then:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi(\\theta|x) &\\sim N\\left(\\frac{\\tau^2}{\\tau^2 +\\sigma^2/n}\\bar{X} + \\frac{\\sigma^2/n}{\\tau^2 + \\sigma^2/n}\\mu_0, \\frac{\\tau^2\\sigma^2/n}{\\tau^2+ \\sigma^2/n}\\right) \\\\\n",
    "& \\sim N(\\frac{6}{6 + 8/18}\\bar{X} + \\frac{8/18}{6 + 8/18}(5), \\frac{6(8/18)}{6+8/18}) \\\\\n",
    "& \\sim N(5.72931, 0.41379)\n",
    "\\end{align}\n",
    "$$\n",
    "Our Bayes estimator will be the posterior mean, 5.72931.\n",
    "\n",
    "The 95% equitailed credible set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f6845a-0f8f-40ca-8c2b-1f6ab51cb720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.468533554544652, 6.990086445455348)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "alpha = 0.05\n",
    "mean = 5.72931\n",
    "var = 0.41379\n",
    "\n",
    "post = ss.norm(loc=mean, scale=var**0.5)\n",
    "\n",
    "post.ppf(alpha / 2), post.ppf(1 - alpha / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be1184-22af-4055-8f90-ded660f916cb",
   "metadata": {},
   "source": [
    "The HPD credible set will be the same for the normal distribution because of symmetry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa761613-6f5d-4c7b-bc87-56758b0cd703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.46853355, 6.99008645])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "\n",
    "def conditions(x, post, alpha):\n",
    "    lwr, upr = x\n",
    "\n",
    "    cond_1 = post.pdf(upr) - post.pdf(lwr)\n",
    "    cond_2 = post.cdf(upr) - post.cdf(lwr) - (1 - alpha)\n",
    "\n",
    "    return cond_1, cond_2\n",
    "\n",
    "\n",
    "fsolve(conditions, (4.5, 7.0), args=(post, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb4e4b",
   "metadata": {},
   "source": [
    "(b) Find the posterior probability of hypothesis $H_0 : \\mu \\leq 5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c931b0d2-48a1-4eae-b161-a383f4bb9844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12844704607549606"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.cdf(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256a824-049a-4ddc-8f25-e5f318bc7479",
   "metadata": {},
   "source": [
    "(c) What is your prediction for a single future observation?\n",
    "\n",
    "Since we only want a single future observation, we can use this from Greg's U4L13 notes (linked under the lecture videos):\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = \\int_\\theta \\mu(\\theta) \\pi(\\theta \\mid x_i) d\\theta\n",
    "$$\n",
    "\n",
    "where $\\mu(\\theta) = \\mathbb{E}[X] = \\int x f(x \\mid \\theta) dx$ is the mean of the original likelihood (the distribution of $X|\\theta$).\n",
    "\n",
    "We can use the [```.expect()```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.expect.html) method on our posterior to calculate this single value, since it will take the expectation of a function with respect to our posterior. Although in the model we treat the mean of the likelihood as unknown, here we can use the mean of $\\mu$ (the mean of the prior on $\\mu$).\n",
    "\n",
    "As the original hints for the solution said, this will equal the posterior mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e5bddec-4fe7-4bfa-8d28-a4d7cdd94791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.729310000000003"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define likelihood\n",
    "lik = ss.norm(loc=5, scale=8**0.5)\n",
    "\n",
    "post.expect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df329374-a2f2-49b1-a05e-f8b6e1911589",
   "metadata": {},
   "source": [
    "A PyMC solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4681b9e-64d8-47e2-a27b-6218a58009e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='24000' class='' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [24000/24000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 1 seconds.\n",
      "Sampling: [lik]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='20000' class='' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [20000/20000 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "with pm.Model() as m:\n",
    "    mu_prior = pm.Normal(\"mu\", 5, sigma=6**0.5)\n",
    "\n",
    "    likelihood = pm.Normal(\"lik\", mu_prior, sigma=8**0.5, observed=data)\n",
    "\n",
    "    trace = pm.sample(5000)\n",
    "    pm.sample_posterior_predictive(trace, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c1694a6-89a7-41ea-8991-80b2d7b23491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>5.721</td>\n",
       "      <td>0.647</td>\n",
       "      <td>4.503</td>\n",
       "      <td>7.033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean     sd  hdi_2.5%  hdi_97.5%\n",
       "mu  5.721  0.647     4.503      7.033"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(trace, hdi_prob=0.95, kind=\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bc2044f-1b77-485d-9981-04ca76ed4ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray ()>\n",
      "array(5.72300883)\n"
     ]
    }
   ],
   "source": [
    "print(trace.posterior_predictive.to_array().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c10ba-91e0-4382-bfb9-b64653c1af54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b2b59a",
   "metadata": {},
   "source": [
    "## 15. Jeremy and Poisson\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aa81a",
   "metadata": {},
   "source": [
    "## 16. NPEB for *p* in the Geometric Distribution\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0041",
   "metadata": {},
   "source": [
    "## 17. Lifetimes and Predictive Distribution\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7814",
   "metadata": {},
   "source": [
    "## 18. Normal Likelihood with Improper Priors\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06308b3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py,md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
