{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a1530a-d066-4c0d-9f96-a18fdf26507d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Ingredients for Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3e95f-bcd5-4e0d-a907-10d63176037e",
   "metadata": {},
   "source": [
    "Let's start with Bayes' theorem again:\n",
    "\n",
    "$$\\pi(\\theta \\mid x) = \\frac{f(x \\mid \\theta) \\pi(\\theta)}{m(x)}$$\n",
    "\n",
    "This is the notation we'll use when talking about probability distributions rather than events as we've done in Unit 3.\n",
    "\n",
    "## $\\pi(\\theta \\mid x)$: the posterior distribution \n",
    "\n",
    "This is the prior updated by the data and normalized.\n",
    "\n",
    "## $f(x \\mid \\theta)$: the likelihood \n",
    "Contains all the information about our experiment. It's an observed variable, so we describe it with $f(\\cdot)$ rather than $\\pi(\\cdot)$ like the posterior and prior. The likelihood is:\n",
    "\n",
    "$$L(\\theta|x_1, ..., x_n) = \\prod_{i=1}^{n} f(x_i|\\theta)$$\n",
    "\n",
    "Where $n$ is the number of datapoints. As the sample size increases, the likelihood tends to dominate the prior, leading to a posterior more influenced by the data. Put another way, as we get more data, our prior beliefs become less important, so the data drives our conclusions.\n",
    "\n",
    "## $\\pi(\\theta)$: the prior \n",
    "Usually we use this to describe the state of our knowledge or expert opinion prior to the experiment. Every unobserved variable in your model is a parameter. For each parameter, you will need to elicit a prior. Some students have criticized using $\\pi(\\cdot)$ to describe both the posterior and the prior, but that is intentional. It's accurately describing what we're doing with Bayes' theoremâ€”updating our priors with new information to form our posterior.\n",
    "\n",
    "## $m(x)$: the marginal distribution or normalizing constant \n",
    "Obtain it by integrating the joint distribution of $x$ and $\\theta$ over all possible values of $\\theta$:\n",
    "\n",
    "$$m(x) = \\int_{\\Theta} f(x|\\theta) \\pi(\\theta) d\\theta$$\n",
    "\n",
    "This integral is often intractable, which is why a large portion of this course is devoted to strategies to avoid calculating it. More on that later; but because of this situation, we often just write Bayes' theorem in its proportional form:\n",
    "\n",
    "$$\\pi(\\theta|x) \\propto f(x|\\theta) \\pi(\\theta)$$\n",
    "\n",
    "The $\\propto$ means \"proportional to.\" Often this is all you need, whether you're going for a ratio (where the marginal cancels out) or you can recognize the kernel of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a887746-3e61-4089-8bcc-a92940f0eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
