{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2652fbd5-a218-41e0-ad70-d1b4fd972bb8",
   "metadata": {},
   "source": [
    "# 3. Laplace's Method\n",
    "\n",
    "## Lecture errata\n",
    "\n",
    "There are two mistakes in the lecture slides, and one extra result (2) is provided:\n",
    "\n",
    "1. In the expression for univariate $Q$, we should retain the negative sign: it should be\n",
    "\n",
    "$$Q=\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log\\left(g\\left(\\theta\\right)\\right)\\right] $$\n",
    "\n",
    "instead of\n",
    "\n",
    "$$Q=\\left[\\frac{\\partial^2}{\\partial\\theta^2}\\log\\left(g\\left(\\theta\\right)\\right)\\right]$$\n",
    "\n",
    "2. We know that $r+ \\alpha -1>0$ because $\\frac{d}{d \\theta} \\log(g(\\theta))=\\frac{r+ \\alpha -1}{\\theta} - (\\beta +x) =0$ $\\rightarrow$ $\\frac{r+ \\alpha -1}{\\theta} =\\beta +x$, and since $\\beta,x,\\theta >0$ the result follows.\n",
    "\n",
    "3. The second derivative of $\\log(g(\\theta))$ should be $-\\frac{r+\\alpha-1}{\\theta^2}$ not $-\\frac{r+\\alpha-1}{(x+\\beta)^2}$. The important point is that the second derivative is negative due to (2), so the value obtained for $\\hat{\\theta}$ is a maximum.\n",
    "\n",
    "All credit goes to Head TA Greg for these lecture corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6088a11-bfcb-45d7-a576-899ccbdf60c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes\n",
    "\n",
    "Laplace's method is another integral approximation technique. This is faster than MCMC, but not as flexible.\n",
    "\n",
    "We expand the log of the function around its mode in a second-order [Taylor expansion](https://www.youtube.com/watch?v=3d6DsjIBzJ4). This process results in a quadratic approximation of the function in the log space, which translates to a normal approximation in the original space.\n",
    "\n",
    "This method is only useful for unimodal, twice-differentiable distributions that aren't too skewed. You can extend the concept to apply to more challenging distributions with [Integrated Nested Laplace Approximations (INLA)](https://en.wikipedia.org/wiki/Integrated_nested_Laplace_approximations), but we don't go into that in this course.\n",
    "\n",
    "Say we have an un-normalized posterior $g(\\theta|x)$. We can't normalize it for whatever reason, so we'll try to approximate it using a normal distribution:\n",
    "\n",
    "$$g(\\theta|x) \\approx N(\\hat{\\theta},Q^{-1})$$\n",
    "\n",
    "A normal distribution is defined by mean and variance (or precision), so we need to find these parameters. \n",
    "\n",
    "From Bayes' theorem, we know:\n",
    "\n",
    "$$g(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{\\int f(x|\\theta)\\pi(\\theta)d\\theta}$$\n",
    "\n",
    "We want to approximate the joint log-likelihood, so let's manipulate the above a bit:\n",
    "\n",
    "$$g(\\theta|x) = \\frac{e^{\\log\\left(f(x|\\theta)\\pi(\\theta)\\right)}}{\\int e^{\\log\\left( f(x|\\theta)\\pi(\\theta)\\right)}d\\theta}$$\n",
    "\n",
    "The joint log-likelihood is $\\log\\left(f(x|\\theta)\\pi(\\theta)\\right)$. Let's shorten that to $h(\\theta)$ for now. We're going to approximate it with the second-order Taylor expansion, which is:\n",
    "\n",
    "$$h(\\theta) \\approx h(\\hat{\\theta}) + (\\theta - \\hat{\\theta})^T \\nabla h(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})$$\n",
    "\n",
    "in the multivariate case, or \n",
    "\n",
    "$$h(\\theta) \\approx h(\\hat{\\theta}) + h'(\\hat{\\theta})(\\theta - \\hat{\\theta}) + \\frac{1}{2}h''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2$$\n",
    "\n",
    "in the univariate case. We can drop the first-order term since it will always evaluate to 0 at the MAP solution, which leaves us with \n",
    "\n",
    "$$h(\\theta) \\approx h(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})$$\n",
    "\n",
    "The first-order term will drop out since it the gradient or slope of the posterior evaluated at the MAP will be zero, leaving us with:\n",
    "\n",
    "$$h(\\theta) \\approx h(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})$$\n",
    "\n",
    "Let's plug that back in to the equation above:\n",
    "\n",
    "$$g(\\theta|x) = \\frac{e^{h(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta -\\hat{\\theta})}}{\\int e^{h(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})} d\\theta}$$\n",
    "\n",
    "We can pull out the first term, which is a constant.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g(\\theta|x) & = \\frac{e^{h(\\hat{\\theta})}e^{\\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta -\\hat{\\theta})}}{e^{h(\\hat{\\theta})}\\int e^{\\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})\\, }d\\theta} \\\\\n",
    "& = \\frac{e^{\\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta -\\hat{\\theta})}}{\\int e^{\\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 h(\\hat{\\theta})(\\theta - \\hat{\\theta})\\, }d\\theta}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To make this match up with the lecture notation, let's let $Q = -\\left(\\nabla^2 h(\\hat{\\theta})\\right) = -\\left[\\nabla^2 \\log\\left(f(x|\\hat{\\theta})\\pi(\\hat{\\theta})\\right)\\right]$. Then:\n",
    "\n",
    "$$g(\\theta|x) = \\frac{1}{\\int e^{-\\frac{1}{2}(\\theta - \\hat{\\theta})^T Q(\\theta - \\hat{\\theta})\\, }d\\theta} e^{-\\frac{1}{2}(\\theta - \\hat{\\theta})^T Q(\\theta -\\hat{\\theta})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982c61c-74fe-4b4a-a29d-619b6ac6f59b",
   "metadata": {},
   "source": [
    "The multivariate normal distribution is specified as:\n",
    "\n",
    "$$\n",
    "f(x | \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} e^{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)}\n",
    "$$\n",
    "\n",
    "Where $|\\Sigma|$ is the determinant of $\\Sigma$. We can see that the forms are very similar, with different normalizing constants. We'll check how well our $N(\\hat{\\theta},Q^{-1})$ approximation works with some examples in the next lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
