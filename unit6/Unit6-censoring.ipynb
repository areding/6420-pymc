{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39320391",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pymc as pm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ccff04",
   "metadata": {},
   "source": [
    "# Type-1 Censoring*\n",
    "\n",
    "Censoring occurs when we have incomplete information about the value of an observation. That is, instead of observing the exact value of a variable, we only know that it falls above, below, or between certain thresholds.  This is common in many fields:\n",
    "\n",
    " - In survival analysis, if a patient leaves a study before the event (e.g., death) occurs, we only know their survival time exceeds the time they were last seen.\n",
    "\n",
    " - In industrial testing, sometimes instruments can only measure up to a certain maximum (or minimum).\n",
    " \n",
    "     - A scale might return a value of \"too heavy\", over a certain weight threshold.  This is known as **right censoring**.\n",
    "\n",
    "     - A water testing kit that unable to detect a chemical substance if the concentration is below $x$ parts per million (ppm) might return \"not detected\".  This is an example of **left censoring**.\n",
    "\n",
    "     - Notice that, in both cases, though we cannot observe the value exactly, we are resonably certain it is above (right) or below (left) a certain threshold.\n",
    "     \n",
    "      \n",
    "\n",
    "\n",
    "## Bartholomew (1957)\n",
    "\n",
    "This is our first introduction to censoring.  It is adapted from [Unit 6: exponential1.odc](https://raw.githubusercontent.com/areding/6420-pymc/main/original_examples/Codes4Unit6/exponential1.odc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc88e79",
   "metadata": {},
   "source": [
    "### Results of a life test on ten pieces of equipment\n",
    "\n",
    "The observations above are the result of a test on the lifespan of some piece of equipment. The test had to end on August 31, but items 2, 4, and 10 did not fail by that date (the values in parentheses are the eventual lifespans of those pieces of equipment, but they were not known on August 31). So all we know for the purposes of the experiment is that they worked for 72, 60, and 21 days, respectively, and that they will continue working for some unknown additional time period.\n",
    "\n",
    "Dataset and table from: [A Problem in Life Testing (Bartholemew 1957)](https://www.jstor.org/stable/2280905?seq=1#metadata_info_tab_contents).\n",
    "\n",
    "<div align='center'>\n",
    "\n",
    "| Item No.           | 1       | 2        | 3       | 4       | 5       | 6       | 7       | 8       | 9       | 10      |\n",
    "|--------------------|---------|----------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
    "| Date of Installation | 11 June | 21 June  | 22 June | 2 July  | 21 July | 31 July | 31 July | 1 Aug.  | 2 Aug.  | 10 Aug. |\n",
    "| Date of Failure    | 13 June | —        | 12 Aug. | —       | 23 Aug. | 27 Aug. | 14 Aug. | 25 Aug. | 6 Aug.  | —       |\n",
    "| Life in Days ($t_i$)         | 2       | (119)    | 51      | (77)    | 33      | 27      | 14      | 24      | 4       | (37)    |\n",
    "| No. Days to End of Period ($T_i$) | 81      | 72       | 70      | 60      | 41      | 31      | 31      | 30      | 29      | 21      |\n",
    "\n",
    "</div>\n",
    "\n",
    "The goal of the experiment is to provide a lifespan estimate. We could:\n",
    "\n",
    "1. Take the censored data as observed:\n",
    "    - Divide the total observed working time (308 days) and divide by the equipment count (10) to get an average lifetime of 308 for an estimate of 30.8 days.\n",
    "    - Problem: underestimates actual average lifespan.\n",
    "\n",
    "2. Ignore the equipment that didn't fail:\n",
    "    - Take mean lifetime of the pieces of equipment that broke within the experiment period for an estimate of 22.1 days.\n",
    "    - Problems: selection bias, underestimates even more.\n",
    "\n",
    "3. Use the classical method:\n",
    "    - Maximum Likelihood Estimation (MLE) under an exponential model. Total observed lifetime divided by 7 (number of observed failures) for an estimate of 44.0 days.\n",
    "    - Problem: small sample size.\n",
    "\n",
    "The actual mean lifespan of all pieces of equipment was 38.8 days.\n",
    "\n",
    "### A Bayesian Approach : Two Ways\n",
    "\n",
    "\n",
    "We will right-censor the three machines that still hadn't failed by August 31, following [this example](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/GLM-truncated-censored-regression.html).\n",
    "\n",
    "We specify our priors as above, and use `pm.Deterministic` for $\\frac{1}{\\lambda}$.  Notice we create introduce an exponential distribution *without* supplying an observed argument.  Then, wrapping that in `pm.Censored` with an `observed` argument tells PyMC that this is a likelihood with censoring.  Specifying combinations of `lower` and `upper` arguments will give the desired censoring:\n",
    "\n",
    " - `lower=None` and `upper` specified : *right* censoring\n",
    "\n",
    " - `lower` specified and `upper=None` : *left* censoring\n",
    "\n",
    " - both `lower` and `upper` specified: *interval* censoring\n",
    "\n",
    " #### Imputing Censored Points\n",
    "\n",
    " We can impute the 3 censored points by using a left-truncated version of the latent distribution. This makes sense because we know the lifetime reached the censored value, and then we sample the latent distribution on the right of each censored point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e774a05c",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [λ, censored_imputed]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49279c9ebb524c599b0b264684a4bfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# gamma dist parameters\n",
    "α = 0.01\n",
    "β = 0.1\n",
    "\n",
    "# max possible observed life for each piece of equipment (before end of experiment)\n",
    "censored = (81, 72, 70, 60, 41, 31, 31, 30, 29, 21)\n",
    "\n",
    "# observed life within experiment dates\n",
    "y = (2, 72, 51, 60, 33, 27, 14, 24, 4, 21)\n",
    "\n",
    "\n",
    "with pm.Model() as m:\n",
    "    λ = pm.Gamma(\"λ\", α, β)\n",
    "    μ = pm.Deterministic(\"μ\", 1 / λ)\n",
    "    obs_latent = pm.Exponential.dist(λ)\n",
    "\n",
    "    observed = pm.Censored(\n",
    "        \"observed\",\n",
    "        obs_latent,\n",
    "        lower=None,\n",
    "        upper=censored,\n",
    "        observed=y,\n",
    "        shape=len(y),\n",
    "    )\n",
    "\n",
    "    #impute censored points\n",
    "    censored_imputed = pm.Truncated(\"censored_imputed\",obs_latent,lower=[72,60,21])\n",
    "\n",
    "    trace = pm.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4daf8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_2.5%</th>\n",
       "      <th>hdi_97.5%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>censored_imputed[0]</th>\n",
       "      <td>123.111</td>\n",
       "      <td>61.181</td>\n",
       "      <td>72.000</td>\n",
       "      <td>234.111</td>\n",
       "      <td>0.580</td>\n",
       "      <td>1.443</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>6434.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>censored_imputed[1]</th>\n",
       "      <td>111.495</td>\n",
       "      <td>61.776</td>\n",
       "      <td>60.008</td>\n",
       "      <td>223.221</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.360</td>\n",
       "      <td>10197.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>censored_imputed[2]</th>\n",
       "      <td>71.592</td>\n",
       "      <td>58.517</td>\n",
       "      <td>21.001</td>\n",
       "      <td>181.962</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.152</td>\n",
       "      <td>11514.0</td>\n",
       "      <td>8874.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>λ</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11541.0</td>\n",
       "      <td>9435.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>μ</th>\n",
       "      <td>50.986</td>\n",
       "      <td>22.636</td>\n",
       "      <td>18.702</td>\n",
       "      <td>93.074</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.452</td>\n",
       "      <td>11541.0</td>\n",
       "      <td>9435.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean      sd  hdi_2.5%  hdi_97.5%  mcse_mean  mcse_sd  \\\n",
       "censored_imputed[0]  123.111  61.181    72.000    234.111      0.580    1.443   \n",
       "censored_imputed[1]  111.495  61.776    60.008    223.221      0.541    1.360   \n",
       "censored_imputed[2]   71.592  58.517    21.001    181.962      0.508    1.152   \n",
       "λ                      0.023   0.009     0.008      0.040      0.000    0.000   \n",
       "μ                     50.986  22.636    18.702     93.074      0.240    0.452   \n",
       "\n",
       "                     ess_bulk  ess_tail  r_hat  \n",
       "censored_imputed[0]    9004.0    6434.0    1.0  \n",
       "censored_imputed[1]   10197.0    7827.0    1.0  \n",
       "censored_imputed[2]   11514.0    8874.0    1.0  \n",
       "λ                     11541.0    9435.0    1.0  \n",
       "μ                     11541.0    9435.0    1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47013c9d",
   "metadata": {},
   "source": [
    "#### The Conjugate Prior Approach:\n",
    "\n",
    "The priors chosen above also allow us to use analytical methods (conjugate priors).  This will give us some intuition for what happens to the likelihood function for censored observations.  We will use an $\\text{Exponential}(\\lambda)$ likelihood, where $\\lambda$ has a $\\text{Gamma}(\\alpha = 0.01,\\beta = 0.1)$ prior distribution.  First, consider the likelihood function for the case that all values were fully observed (no censoring):\n",
    "\n",
    "$$\n",
    "f(Y=y\\vert \\lambda ) = \\prod_{i=1}^{n}\\lambda \\exp\\big\\{-\\lambda y_{i}\\big\\}\n",
    "$$\n",
    "\n",
    "We evaluate the exponential PDF at the observation value and multiply them all together.  Consider now the cases where a failure was not observed.  Note that for these, $T_{i}$ in the table above denotes the point after which a failure would be observed had it occurred. \n",
    "\n",
    "\n",
    "In the censored case, points above the censoring threshold $T_{i}$ will contribute to the likelihood a bit differenly:\n",
    "\n",
    "\\begin{align*}\n",
    "f(Y \\geq T_i \\mid \\lambda)\n",
    "&= \\int_{T_i}^{\\infty} \\lambda \\exp\\big\\{ -\\lambda y \\big\\} \\, dy = \\exp\\big\\{ -\\lambda T_i \\big\\}\n",
    "\\end{align*}\n",
    "\n",
    "The censoring-adjusted likelihood function becomes:\n",
    "\n",
    "$$\n",
    "f(y\\vert \\lambda) = \\prod_{\\{i\\vert y_{i} < T_{i}\\}}\\lambda \\exp\\big\\{-\\lambda y_{i}\\big\\}\\times \\prod_{\\{i\\vert y_{i}\\geq T_{i}\\}}\\exp\\big\\{-\\lambda T_i\\big\\}\n",
    "$$\n",
    "\n",
    "We introduce an indicator variable $\\delta_{i} = 1$ for $y_{i}< T_{i}$, otherwise $\\delta_{i} = 0$.  The posterior distribution is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\pi(\\lambda \\vert \\mathbf{y})&\\propto \\prod_{\\{i\\vert y_{i} < T_{i}\\}}\\lambda \\exp\\big\\{-\\lambda y_{i}\\big\\}\\times \\prod_{\\{i\\vert y_{i}\\geq T_{i}\\}}\\exp\\big\\{-\\lambda T_i\\big\\}\\times \\lambda^{\\alpha - 1}\\exp\\big\\{ - \\lambda \\beta \\big\\}\\\\\n",
    "&\\phantom{;}\\\\ \n",
    "&= \\lambda^{(\\alpha + \\sum_{i}\\delta_{i}) - 1}\\exp\\big\\{-\\lambda (\\beta + \\sum_{\\{i\\vert y_{i} < T_{i}\\}}y_{i} +  \\sum_{\\{i\\vert y_{i} \\geq T_{i}\\}}T_{i})\\big\\}\\\\\n",
    "&\\sim \\text{Gamma}(\\alpha + \\sum_{i=1}^{n}\\delta_{i}\\text{ , } \\beta + \\sum_{\\{i\\vert y_{i} < T_{i}\\}}y_{i} +  \\sum_{\\{i\\vert y_{i} \\geq T_{i}\\}}T_{i})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Take note of the subset indexing that appears in the rate parameter.  We only want to add the $y_{i}$'s that are fully observed.  For observations that are right censored, we add the $T_{i}$'s instead.\n",
    "\n",
    "The posterior distribution for $\\lambda$ (the failure rate) summarizes our updated beliefs about the typical rate at which equipment failures occur, given both the observed and censored data. Lower values of $\\lambda$ indicate a lower failure rate, and longer equipment lifetimes.  Next, for interpretability, let's define $\\mu = \\frac{1}{\\lambda}$.  Then $\\pi(\\mu\\vert \\mathbf{y})$ is a posterior distribution of expected lifetimes until equipment failure.  Next we will:\n",
    "\n",
    " - Calculate the mean and standard deviation of $\\pi(\\lambda \\vert \\mathbf{y})$ using their closed form solutions\n",
    " \n",
    " - Calculate an estimate of the 95% HDI credible interval using `np.random.gamma` samples and `az.hdi`\n",
    "\n",
    " - Calculate the mean, standard deviation, and 95% HDI credible interval for $\\pi(\\mu\\vert \\mathbf{y})$ using the reciprocals of our samples above.\n",
    " \n",
    " \n",
    " An estimate of the mean, standard deviation, and 95% HDI credible interval for $\\pi(\\frac{1}{\\lambda}\\vert \\mathbf{y})$.  This is a posterior distribution for the mean lifetime of the equipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f103d97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ Posterior:\n",
      "-------------------------------------------------------------------------\n",
      "Posterior Mean: 0.0228   Posterior SD: 0.0086   HDI Bounds[0.0078,0.0400]\n",
      "\n",
      "μ = 1/λ Posterior:\n",
      "-----------------------------------------------------------------------------\n",
      "Posterior Mean: 51.2348   Posterior SD: 22.9443   HDI Bounds[19.2470,95.1005]\n"
     ]
    }
   ],
   "source": [
    "#prior parameters:\n",
    "α = 0.01\n",
    "β = 0.1\n",
    "\n",
    "#data:\n",
    "\n",
    "# max possible observed life for each piece of equipment (before end of experiment)\n",
    "T = (81, 72, 70, 60, 41, 31, 31, 30, 29, 21)\n",
    "\n",
    "# observed life within experiment dates\n",
    "Y = (2, 72, 51, 60, 33, 27, 14, 24, 4, 21)\n",
    "\n",
    "ty_paired = list(zip(T,Y))\n",
    "\n",
    "                #prior alpha    +   number of fully observed       \n",
    "α_posterior =   α               +   sum(y<t for t,y in zip(T,Y))  \n",
    "\n",
    "\n",
    "                #prior beta     +    #sum of fully observed values      +    #sum of T's for right censored values ONLY\n",
    "β_posterior =   β               +   sum((y<t)*(y) for t,y in ty_paired) +    sum((y>=t)*(t) for t,y in ty_paired)\n",
    "\n",
    "\n",
    "# Signature: np.random.gamma(shape, scale=1.0, size=None)\n",
    "# Docstring:\n",
    "# gamma(shape, scale=1.0, size=None)\n",
    "# TLDR: need to use \"scale= 1/β_posterior\"\n",
    "\n",
    "samples = np.random.gamma(α_posterior , 1/β_posterior,100000)\n",
    "\n",
    "\n",
    "\n",
    "print('λ Posterior:')\n",
    "hdi = az.hdi(samples,hdi_prob=0.95)\n",
    "summary = f'Posterior Mean: {α_posterior / β_posterior:<.4f}   Posterior SD: {(α_posterior / ((β_posterior)**(2)))**(1/2):<.4f}   HDI Bounds[{hdi[0]:<.4f},{hdi[1]:<.4f}]'\n",
    "print('-'*len(summary))\n",
    "print(summary)\n",
    "print('')\n",
    "print('μ = 1/λ Posterior:')\n",
    "hdi = az.hdi(1/samples,hdi_prob=0.95)\n",
    "summary = f'Posterior Mean: {(1/samples).mean():<.4f}   Posterior SD: {(1/samples).std():<.4f}   HDI Bounds[{hdi[0]:<.4f},{hdi[1]:<.4f}]'\n",
    "print('-'*len(summary))\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3150c2",
   "metadata": {},
   "source": [
    "The results closely match what we found using PyMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5232113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: Wed Jun 11 2025\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.13.3\n",
      "IPython version      : 9.2.0\n",
      "\n",
      "pytensor: 2.30.3\n",
      "\n",
      "numpy: 2.2.6\n",
      "pymc : 5.22.0\n",
      "arviz: 0.21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -p pytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f83884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
