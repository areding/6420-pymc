{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Graph Theory:\n",
    "\n",
    "In this unit, we begin our exploration of Bayesian models. In the second half of the course, we will implement these models using PyMC. Before diving into code, however, we must first introduce some foundational concepts that allow us to express Bayesian models precisely. Among the most essential tools for this purpose are ideas from graph theory.\n",
    "\n",
    "## What is a Graph?\n",
    "\n",
    "A graph is a collection of **nodes** (sometimes called **vertices**) and the **edges** that connect them.  It's helpful to think of a node as representing an entity, and an edge as representing a relationship or interaction between two entities:\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![Some Graphs](../images/unit6_graph_theory.jpg)\n",
    "\n",
    "</div>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <span style=\"font-size:smaller;\"><b>Examples of Graphs:</b>\n",
    "Left: a transit map; Center: a social network; Right: a molecular structure (2-phenylethanol).\n",
    "Each is an everyday construct that can be represented as a graph—a set of nodes and edges.</span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs come in two main forms:\n",
    "\n",
    " - An **undirected graph** has edges that do not have a direction. If there is an edge between nodes \n",
    "$A$ and $B$.  These types of graphs are often used to represent mutual relationships.  Think of 'friendships' in a social network.  \n",
    "\n",
    "- A **directed graph** (digraph) has edges that point from one node to another.  Using social media as an example, with nodes $A$ and $B$ here are some ways we could write relationships between them:\n",
    "\n",
    "     -    $A\\rightarrow B$ : Person $A$ follows person $B$, but person $B$ does *not* follow person $A$.\n",
    "     -    $B\\rightarrow A$ : Person $B$ follows person $A$, but person $A$ does *not* follow person $A$.\n",
    "     -    $A\\leftrightarrow B$ : $A$ and $B$ are friends with each other\n",
    "     -    Note that if every edge in a directed graph is bidirectional $(\\leftrightarrow)$, the structure would behave identically to an undirected graph.\n",
    "\n",
    "Each of these can also be *cyclic* or *acyclic*:\n",
    "\n",
    " - A **cyclic graph** contains at at least one path that allows you to start at a node and return to the same place.  Using websites as an example: If Site $A$ links to Site $B$, Site $B$ links to Site $C$, and Site $C$ links back to Site $A$, that forms a cycle.\n",
    "\n",
    " - An **acyclic graph** has **no cycles**. You cannot return to a starting point by following a sequence of edges.\n",
    "\n",
    " ## Bayesian Models as Graphs:\n",
    "\n",
    "Bayesian models are expressible as *Directed Acyclic Graphs (DAGs)*.  We have previously seen some simple Bayes networks where the nodes are events.  Now, the nodes represent the random variables in the model and edges denote the statistical dependencies between the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Graphs?  Why DAGs?\n",
    "\n",
    "Nodes and edges are an efficient way to track the relationships between variables.  The *absence* of an edge between nodes encodes the assumption of conditional independence between those random variables.  This method is intuitive and compact but also allows us to leverage the Markovian property.  This is useful in inference later.\n",
    "\n",
    "Consider first the graph (U6 L1 Video, 4:23 , Lecture Slide #5 in PDF) $A\\rightarrow B \\rightarrow C$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each arrow between nodes encodes a conditional dependence:\n",
    "\n",
    " - $B$ depends on $A$.  \n",
    " - $C$ depends on $B$\n",
    " - Therefore $B$ influences $C$\n",
    "\n",
    "Note that $A$ cannot *directly* influence $C$.  It can only influence $C$ through $B$.  This structure has important implicatons for how we factor the joint distribution $P(A,B,C)$.\n",
    "\n",
    "Recall that the **chain rule** (or **general product rule**) allows us to decompose the joint probability into a product of conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(X_{1} , X_{2} , \\cdots , X_{n}) = \\prod_{k=1}^{n}P(X_{k} \\vert X_{1} , \\cdots , X_{k-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting this to the Markov Property:\n",
    "\n",
    "In the context of our Bayesian models, the Markov property states that a node is conditionally independent of **the node's non-descendants given the node's parents**.  Applied to our graph:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "| **Explanations:** | **Graph $A\\rightarrow B \\rightarrow C$:** |\n",
    "|--------------------------------|----------------------------------|\n",
    "| **Parent(s)**<br>The immediate nodes that have arrows pointing into a node.<br>| **Variable $A$**<br>• Parents: None<br>• Descendants: $B$, $C$<br>• Non-Descendants : None|\n",
    "| **Descendants**<br>All nodes that can be reached by following directed edges starting from the node.| **Variable B**<br>• Parents: $A$<br>• Descendants: $C$<br>• Non-descendants: $A$|\n",
    "| **Non-descendants**<br>All nodes in the graph except:<br>• the node itself<br>• its descendants | **Variable C**<br>• Parents: $B$<br>• Descendants: None<br>• Non-descendants: $A,B$\n",
    "</div>\n",
    "\n",
    "\n",
    "Note that for node $C$, the Markov property applies: $C \\perp\\!\\!\\!\\perp A \\vert B$.  We take this as ground truth and proceed to show something interesting about $A$.  We can prove that even though we are conditioning on $B$ and $C$, the additional information from $C$ is redundant once $B$ is known.  We can do this using only Bayes' Rule, Conditional Probability, and the chain rule from above:  Consider $P(A\\vert B,C)$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A \\mid B, C) \n",
    "    &= \\frac{P(B, C \\mid A) \\cdot P(A)}{P(B, C)} \n",
    "    && \\leftarrow \\textcolor{blue}{\\text{Bayes' Rule}} \\\\\n",
    "    &= \\frac{P(A, B, C)}{P(B, C)} \n",
    "    && \\leftarrow \\textcolor{blue}{\\text{Conditional Probability}} \\\\\n",
    "    &= \\frac{P(A) \\cdot P(B \\mid A) \\cdot P(C \\mid B)}{P(B, C)} \n",
    "    && \\leftarrow \\textcolor{blue}{\\text{Chain rule/general product rule}} \\\\\n",
    "    &= \\frac{P(A) \\cdot P(B \\mid A) \\cdot \\cancel{P(C \\mid B})}{\\cancel{P(C \\mid B)} \\cdot P(B)} \n",
    "    && \\leftarrow \\textcolor{blue}{\\text{Conditional Probability}} \\\\\n",
    "    &= \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\\\\ \n",
    "    &= P(A \\mid B)\\square\n",
    "    && \\leftarrow \\textcolor{blue}{\\text{Bayes' Rule}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Applying both the Markov Property and the chain rule gives us:\n",
    "\n",
    "$$\n",
    "P(ABC) = P(A)\\times P(B\\vert A) \\times P(C\\vert B)\n",
    "$$\n",
    "\n",
    "What we have observed here is a simple case of **$d$-separation**: a graphical criterion that determines when two sets of nodes are conditionally independent, given a third set.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
