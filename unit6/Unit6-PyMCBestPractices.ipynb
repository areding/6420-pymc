{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMC Best Practices\n",
    "\n",
    "PyMC is a powerful library for probabilistic programming in Python, allowing you to define and fit Bayesian statistical models. This document outlines some best practices to help you write efficient, readable, and reliable PyMC code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Context Managers for Model Definition\n",
    "\n",
    "Defining models within a context manager (with pm.Model() as model:) ensures that all variables and distributions are properly associated with the model. This practice helps avoid confusion when working with multiple models and makes your code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Define priors, likelihood, etc.\n",
    "    pass  # Your model code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data Containers (<span style=\"background-color: gray;\">pm.math.dot</span>)\n",
    "\n",
    "Using data containers helps in managing data within your model and allows for easy updates without redefining the entire model. This is particularly useful when performing posterior predictive checks or updating models with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.linspace(0, 1, 100)\n",
    "Y = 2 * X + np.random.normal(0, 0.1, size=100)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Define data containers\n",
    "    x_shared = pm.Data(\"x_shared\", X)\n",
    "    y_shared = pm.Data(\"y_shared\", Y)\n",
    "    \n",
    "    # Define priors\n",
    "    slope = pm.Normal(\"slope\", mu=0, sigma=1)\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "    \n",
    "    # Define likelihood\n",
    "    mu = intercept + slope * x_shared\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_shared)\n",
    "    \n",
    "    # Sample from the posterior\n",
    "    trace = pm.sample(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the <span style=\"background-color: gray;\">shape</span> Parameter\n",
    "\n",
    "The  <span style=\"background-color: gray;\">shape</span> parameter allows you to define arrays of random variables efficiently, avoiding repetitive code. This is particularly helpful when dealing with multivariate models or models with multiple parameters of the same kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "# Assume we have 3 predictors\n",
    "with pm.Model() as model:\n",
    "    coefficients = pm.Normal(\"coefficients\", mu=0, sigma=1, shape=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyMC's Built-in Dot Product (<span style=\"background-color: gray;\">pm.math.dot</span>)\n",
    "\n",
    "For linear algebra operations, <span style=\"background-color: gray;\">pm.math.dot</span> provides a clean and readable way to perform dot products within your model. This is especially useful when utilizing the shape parameter for vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data\n",
    "X = np.random.randn(100, 3)\n",
    "y = X @ np.array([1.5, -2.0, 0.5]) + np.random.randn(100)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Data containers\n",
    "    X_shared = pm.Data(\"X_shared\", X)\n",
    "    y_shared = pm.Data(\"y_shared\", y)\n",
    "    \n",
    "    # Priors\n",
    "    coefficients = pm.Normal(\"coefficients\", mu=0, sigma=1, shape=3)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "    \n",
    "    # Linear algebra with pm.math.dot\n",
    "    mu = pm.math.dot(X_shared, coefficients)\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_shared)\n",
    "    \n",
    "    # Sampling\n",
    "    trace = pm.sample(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyMC's Observe Functionality (<span style=\"background-color: gray;\">pm.observe</span>)\n",
    "\n",
    "The (<span style=\"background-color: gray;\">pm.observe</span>) function allows you to pass data to your model in a flexible way, making it easier to test multiple models or datasets without redefining the entire model each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "# Model 1\n",
    "# Weakly informative prior\n",
    "with pm.Model() as m1:\n",
    "    x = pm.Normal(\"x\", mu=0, sigma=10000)\n",
    "    y = pm.Normal.dist(x, shape=(5,))\n",
    "    y_censored = pm.Deterministic(\"y_censored\", pm.math.clip(y, -1, 1))\n",
    "\n",
    "# Model 2\n",
    "# Tighter informative prior\n",
    "with pm.Model() as m2:\n",
    "    x = pm.Normal(\"x\", mu=10, sigma=2)\n",
    "    y = pm.Normal.dist(x, shape=(5,))\n",
    "    y_censored = pm.Deterministic(\"y_censored\", pm.math.clip(y, -1, 1))\n",
    "\n",
    "# Test 1\n",
    "new_m1 = pm.observe(m1, {y_censored: [0.9, 0.5, 0.3, 1, 1]})\n",
    "\n",
    "# Test 2\n",
    "new_m2 = pm.observe(m2, {y_censored: [0.9, 0.5, 0.3, 1, 1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization for Efficiency\n",
    "\n",
    "This is general advice for this class and future classes. \n",
    "\n",
    "Using vectorized operations can significantly speed up your model, especially with large datasets. Vectorization reduces the overhead of Python loops and leverages optimized numerical libraries.\n",
    "\n",
    "NumPy is the general-use library for vectorized operations. JAX provides a powerful tool called Autograd that lets you put gradients on the GPU, significantly speeding up compute time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "# Large dataset\n",
    "X = np.random.randn(10000, 3)\n",
    "y = X @ np.array([1.5, -2.0, 0.5]) + np.random.randn(10000)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    coefficients = pm.Normal(\"coefficients\", mu=0, sigma=1, shape=3)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "    mu = pm.math.dot(X, coefficients)\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n",
    "    trace = pm.sample()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
